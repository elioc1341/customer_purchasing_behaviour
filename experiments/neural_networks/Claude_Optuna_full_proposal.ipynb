{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1454a24",
   "metadata": {},
   "source": [
    "Claude Bot - Optuna Full Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2eb33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MLFlow + Optuna + KFold + Adaptive Learning Neural Network Training\n",
    "# ====================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import optuna\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ====================================================================\n",
    "# 1. NEURAL NETWORK MODEL DEFINITION\n",
    "# ====================================================================\n",
    "\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate=0.0):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# ====================================================================\n",
    "# 2. GRADIENT ADAPTIVE BATCHING LEARNING CLASS\n",
    "# ====================================================================\n",
    "\n",
    "class GradientAdaptiveBatchingLearning:\n",
    "    def __init__(self, initial_lr=0.005, lr_min=0.0005, initial_batch_size=16,\n",
    "                 grad_threshold=1.0, batch_multiplier=2, lr_multiplier=0.5, \n",
    "                 patience=10, max_batch_size=None):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.lr_min = lr_min\n",
    "        self.initial_batch_size = initial_batch_size\n",
    "        self.grad_threshold = grad_threshold\n",
    "        self.batch_multiplier = batch_multiplier\n",
    "        self.lr_multiplier = lr_multiplier\n",
    "        self.patience = patience\n",
    "        self.max_batch_size = max_batch_size\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.current_lr = initial_lr\n",
    "        self.current_batch_size = initial_batch_size\n",
    "        self.loss_history = []\n",
    "        self.lr_history = []\n",
    "        self.batch_size_history = []\n",
    "        self.no_improvement_count = 0\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset adaptive parameters for new training\"\"\"\n",
    "        self.current_lr = self.initial_lr\n",
    "        self.current_batch_size = self.initial_batch_size\n",
    "        self.loss_history = []\n",
    "        self.lr_history = []\n",
    "        self.batch_size_history = []\n",
    "        self.no_improvement_count = 0\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def calculate_loss_slope(self, window_size=5):\n",
    "        \"\"\"Calculate the slope of recent loss values\"\"\"\n",
    "        if len(self.loss_history) < window_size:\n",
    "            return float('-inf')  # Not enough history\n",
    "        \n",
    "        recent_losses = self.loss_history[-window_size:]\n",
    "        x = np.arange(len(recent_losses))\n",
    "        \n",
    "        # Calculate slope using least squares\n",
    "        slope = np.polyfit(x, recent_losses, 1)[0]\n",
    "        return abs(slope)\n",
    "    \n",
    "    def should_adapt(self):\n",
    "        \"\"\"Determine if we should adapt learning parameters\"\"\"\n",
    "        if len(self.loss_history) < self.patience:\n",
    "            return False\n",
    "        \n",
    "        # Check if slope is diminishing\n",
    "        slope = self.calculate_loss_slope()\n",
    "        return slope < self.grad_threshold\n",
    "    \n",
    "    def adapt_parameters(self, dataset_size):\n",
    "        \"\"\"Adapt learning rate and batch size\"\"\"\n",
    "        if self.should_adapt():\n",
    "            # Decrease learning rate\n",
    "            new_lr = max(self.current_lr * self.lr_multiplier, self.lr_min)\n",
    "            \n",
    "            # Increase batch size\n",
    "            new_batch_size = min(\n",
    "                int(self.current_batch_size * self.batch_multiplier),\n",
    "                dataset_size if self.max_batch_size is None else self.max_batch_size\n",
    "            )\n",
    "            \n",
    "            # Only update if there's actually a change\n",
    "            if new_lr != self.current_lr or new_batch_size != self.current_batch_size:\n",
    "                self.current_lr = new_lr\n",
    "                self.current_batch_size = new_batch_size\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def update_history(self, epoch_loss):\n",
    "        \"\"\"Update loss history and tracking\"\"\"\n",
    "        self.loss_history.append(epoch_loss)\n",
    "        self.lr_history.append(self.current_lr)\n",
    "        self.batch_size_history.append(self.current_batch_size)\n",
    "        \n",
    "        if epoch_loss < self.best_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            self.no_improvement_count = 0\n",
    "        else:\n",
    "            self.no_improvement_count += 1\n",
    "\n",
    "# ====================================================================\n",
    "# 3. DATA PREPARATION FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Load your dataset and prepare it for training\n",
    "    Replace this function with your actual data loading logic\n",
    "    \"\"\"\n",
    "    # Placeholder - replace with your actual data loading\n",
    "    # For demonstration, creating synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 238  # Your dataset size\n",
    "    n_features = 9   # Your feature count\n",
    "    \n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.random.randn(n_samples) * 2 + 1  # Some target variable\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def split_and_scale_data(X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into train/val/test and scale features\n",
    "    \"\"\"\n",
    "    # First split: separate test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: separate validation from training\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Data splits - Train: {X_train_scaled.shape[0]}, Val: {X_val_scaled.shape[0]}, Test: {X_test_scaled.shape[0]}\")\n",
    "    \n",
    "    return (X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "            y_train, y_val, y_test, scaler)\n",
    "\n",
    "# ====================================================================\n",
    "# 4. TRAINING FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "def create_data_loader(X, y, batch_size, shuffle=True):\n",
    "    \"\"\"Create PyTorch DataLoader from numpy arrays\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_tensor = torch.FloatTensor(y).view(-1, 1)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, criterion):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_X, batch_y in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    \"\"\"Evaluate model on validation/test data\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "def train_with_adaptive_learning(model, X_train_fold, y_train_fold, X_val_fold, y_val_fold,\n",
    "                               adaptive_trainer, max_epochs=100, patience_early_stop=20):\n",
    "    \"\"\"\n",
    "    Train model with adaptive learning rate and batch size\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=adaptive_trainer.current_lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_count = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Create data loader with current batch size\n",
    "        train_loader = create_data_loader(\n",
    "            X_train_fold, y_train_fold, \n",
    "            batch_size=adaptive_trainer.current_batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = create_data_loader(\n",
    "            X_val_fold, y_val_fold, \n",
    "            batch_size=adaptive_trainer.current_batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Train one epoch\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, criterion)\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update adaptive trainer\n",
    "        adaptive_trainer.update_history(val_loss)\n",
    "        \n",
    "        # Check if we should adapt parameters\n",
    "        if adaptive_trainer.adapt_parameters(len(X_train_fold)):\n",
    "            # Update optimizer learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = adaptive_trainer.current_lr\n",
    "            \n",
    "            print(f\"  Epoch {epoch+1}: Adapted - LR: {adaptive_trainer.current_lr:.6f}, \"\n",
    "                  f\"Batch Size: {adaptive_trainer.current_batch_size}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_count = 0\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            \n",
    "        if patience_count >= patience_early_stop:\n",
    "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'final_val_loss': best_val_loss,\n",
    "        'lr_history': adaptive_trainer.lr_history.copy(),\n",
    "        'batch_size_history': adaptive_trainer.batch_size_history.copy()\n",
    "    }\n",
    "\n",
    "# ====================================================================\n",
    "# 5. K-FOLD CROSS VALIDATION\n",
    "# ====================================================================\n",
    "\n",
    "def perform_kfold_cv(X_train, y_train, model_params, adaptive_params, k_folds=4, \n",
    "                    max_epochs=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform K-Fold cross validation with adaptive learning\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "        print(f\"Training Fold {fold + 1}/{k_folds}\")\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train[train_idx]\n",
    "        X_val_fold = X_train[val_idx]\n",
    "        y_train_fold = y_train[train_idx]\n",
    "        y_val_fold = y_train[val_idx]\n",
    "        \n",
    "        # Create model for this fold\n",
    "        model = FeedforwardNN(\n",
    "            input_size=X_train.shape[1],\n",
    "            hidden_size=model_params['neurons'],\n",
    "            dropout_rate=model_params['dropout']\n",
    "        )\n",
    "        \n",
    "        # Create adaptive trainer for this fold\n",
    "        adaptive_trainer = GradientAdaptiveBatchingLearning(**adaptive_params)\n",
    "        \n",
    "        # Train model\n",
    "        fold_result = train_with_adaptive_learning(\n",
    "            model, X_train_fold, y_train_fold, X_val_fold, y_val_fold,\n",
    "            adaptive_trainer, max_epochs=max_epochs\n",
    "        )\n",
    "        \n",
    "        fold_result['fold'] = fold + 1\n",
    "        fold_results.append(fold_result)\n",
    "        \n",
    "        print(f\"  Fold {fold + 1} final validation loss: {fold_result['final_val_loss']:.6f}\")\n",
    "    \n",
    "    # Calculate average performance\n",
    "    avg_val_loss = np.mean([result['final_val_loss'] for result in fold_results])\n",
    "    std_val_loss = np.std([result['final_val_loss'] for result in fold_results])\n",
    "    \n",
    "    print(f\"Average CV Loss: {avg_val_loss:.6f} ± {std_val_loss:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'avg_cv_loss': avg_val_loss,\n",
    "        'std_cv_loss': std_val_loss\n",
    "    }\n",
    "\n",
    "# ====================================================================\n",
    "# 6. PLOTTING FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "def plot_cv_results(cv_results):\n",
    "    \"\"\"Plot cross-validation results\"\"\"\n",
    "    fold_results = cv_results['fold_results']\n",
    "    n_folds = len(fold_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('K-Fold Cross Validation Results', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Loss curves for each fold\n",
    "    axes[0, 0].set_title('Loss Curves by Fold')\n",
    "    for i, result in enumerate(fold_results):\n",
    "        epochs = range(1, len(result['train_losses']) + 1)\n",
    "        axes[0, 0].plot(epochs, result['train_losses'], \n",
    "                       label=f'Fold {i+1} Train', alpha=0.7)\n",
    "        axes[0, 0].plot(epochs, result['val_losses'], \n",
    "                       label=f'Fold {i+1} Val', alpha=0.7, linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Plot 2: Final validation losses\n",
    "    axes[0, 1].set_title('Final Validation Loss by Fold')\n",
    "    fold_nums = [r['fold'] for r in fold_results]\n",
    "    final_losses = [r['final_val_loss'] for r in fold_results]\n",
    "    axes[0, 1].bar(fold_nums, final_losses, alpha=0.7)\n",
    "    axes[0, 1].axhline(y=cv_results['avg_cv_loss'], color='red', \n",
    "                      linestyle='--', label=f\"Average: {cv_results['avg_cv_loss']:.4f}\")\n",
    "    axes[0, 1].set_xlabel('Fold')\n",
    "    axes[0, 1].set_ylabel('Final Validation Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Plot 3: Learning rate evolution\n",
    "    axes[1, 0].set_title('Learning Rate Evolution')\n",
    "    for i, result in enumerate(fold_results):\n",
    "        epochs = range(1, len(result['lr_history']) + 1)\n",
    "        axes[1, 0].plot(epochs, result['lr_history'], \n",
    "                       label=f'Fold {i+1}', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Plot 4: Batch size evolution\n",
    "    axes[1, 1].set_title('Batch Size Evolution')\n",
    "    for i, result in enumerate(fold_results):\n",
    "        epochs = range(1, len(result['batch_size_history']) + 1)\n",
    "        axes[1, 1].plot(epochs, result['batch_size_history'], \n",
    "                       label=f'Fold {i+1}', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Batch Size')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_optuna_optimization_history(study):\n",
    "    \"\"\"Plot Optuna optimization history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Optimization history\n",
    "    trials = study.trials_dataframe()\n",
    "    axes[0].plot(trials['number'], trials['value'], 'b-', alpha=0.7)\n",
    "    axes[0].set_xlabel('Trial')\n",
    "    axes[0].set_ylabel('Objective Value (CV Loss)')\n",
    "    axes[0].set_title('Optimization History')\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot 2: Parameter importance (if enough trials)\n",
    "    if len(trials) > 10:\n",
    "        try:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            params = list(importance.keys())\n",
    "            values = list(importance.values())\n",
    "            \n",
    "            axes[1].barh(params, values)\n",
    "            axes[1].set_xlabel('Importance')\n",
    "            axes[1].set_title('Parameter Importance')\n",
    "        except:\n",
    "            axes[1].text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
    "                        ha='center', va='center', transform=axes[1].transAxes)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'Not enough trials for\\nparameter importance', \n",
    "                    ha='center', va='center', transform=axes[1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ====================================================================\n",
    "# 7. MLFLOW INTEGRATION\n",
    "# ====================================================================\n",
    "\n",
    "def setup_mlflow(experiment_name=\"neural_network_optimization\"):\n",
    "    \"\"\"Setup MLflow experiment\"\"\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"MLflow experiment: {experiment_name}\")\n",
    "\n",
    "def log_cv_results_to_mlflow(cv_results, trial_number=None):\n",
    "    \"\"\"Log cross-validation results to MLflow\"\"\"\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"avg_cv_loss\", cv_results['avg_cv_loss'])\n",
    "    mlflow.log_metric(\"std_cv_loss\", cv_results['std_cv_loss'])\n",
    "    \n",
    "    # Log individual fold results\n",
    "    for i, fold_result in enumerate(cv_results['fold_results']):\n",
    "        mlflow.log_metric(f\"fold_{i+1}_final_loss\", fold_result['final_val_loss'])\n",
    "    \n",
    "    # Create and log plots\n",
    "    fig = plot_cv_results(cv_results)\n",
    "    mlflow.log_figure(fig, f\"cv_results_trial_{trial_number}.png\" if trial_number else \"cv_results.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# ====================================================================\n",
    "# 8. OPTUNA INTEGRATION\n",
    "# ====================================================================\n",
    "\n",
    "def create_optuna_objective(X_train, y_train):\n",
    "    \"\"\"Create Optuna objective function\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        model_params = {\n",
    "            'neurons': trial.suggest_int('neurons', 8, 64),\n",
    "            'dropout': trial.suggest_float('dropout', 0.0, 0.5)\n",
    "        }\n",
    "        \n",
    "        adaptive_params = {\n",
    "            'initial_lr': trial.suggest_float('initial_lr', 1e-4, 1e-1, log=True),\n",
    "            'lr_min': trial.suggest_float('lr_min', 1e-6, 1e-3, log=True),\n",
    "            'initial_batch_size': trial.suggest_categorical('initial_batch_size', [8, 16, 32]),\n",
    "            'grad_threshold': trial.suggest_float('grad_threshold', 0.0001, 0.01, log=True),\n",
    "            'batch_multiplier': trial.suggest_float('batch_multiplier', 1.5, 3.0),\n",
    "            'lr_multiplier': trial.suggest_float('lr_multiplier', 0.3, 0.8),\n",
    "            'patience': trial.suggest_int('patience', 5, 20)\n",
    "        }\n",
    "        \n",
    "        # Start MLflow run for this trial\n",
    "        with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True):\n",
    "            # Log trial parameters\n",
    "            mlflow.log_params({**model_params, **adaptive_params})\n",
    "            mlflow.log_param(\"trial_number\", trial.number)\n",
    "            \n",
    "            # Perform cross-validation\n",
    "            cv_results = perform_kfold_cv(\n",
    "                X_train, y_train, \n",
    "                model_params, adaptive_params,\n",
    "                k_folds=4, max_epochs=100\n",
    "            )\n",
    "            \n",
    "            # Log results to MLflow\n",
    "            log_cv_results_to_mlflow(cv_results, trial.number)\n",
    "            \n",
    "            return cv_results['avg_cv_loss']\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def run_optuna_optimization(X_train, y_train, n_trials=50):\n",
    "    \"\"\"Run Optuna optimization\"\"\"\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # Create objective\n",
    "    objective = create_optuna_objective(X_train, y_train)\n",
    "    \n",
    "    # Start parent MLflow run\n",
    "    with mlflow.start_run(run_name=\"optuna_hyperparameter_search\"):\n",
    "        # Log study parameters\n",
    "        mlflow.log_params({\n",
    "            'n_trials': n_trials,\n",
    "            'direction': 'minimize',\n",
    "            'sampler': 'TPE'\n",
    "        })\n",
    "        \n",
    "        # Optimize\n",
    "        print(f\"Starting Optuna optimization with {n_trials} trials...\")\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        # Log best results\n",
    "        mlflow.log_params({f\"best_{k}\": v for k, v in study.best_params.items()})\n",
    "        mlflow.log_metric(\"best_cv_loss\", study.best_value)\n",
    "        \n",
    "        # Create and log optimization plots\n",
    "        fig = plot_optuna_optimization_history(study)\n",
    "        mlflow.log_figure(fig, \"optuna_optimization_history.png\")\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best CV loss: {study.best_value:.6f}\")\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "# ====================================================================\n",
    "# 9. MAIN EXECUTION\n",
    "# ====================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=== Neural Network Hyperparameter Optimization ===\")\n",
    "    \n",
    "    # Setup MLflow\n",
    "    setup_mlflow(\"neural_network_adaptive_learning\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n1. Loading and preparing data...\")\n",
    "    X, y = load_and_prepare_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, scaler = split_and_scale_data(X, y)\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\n2. Starting hyperparameter optimization...\")\n",
    "    study = run_optuna_optimization(X_train, y_train, n_trials=20)  # Adjust n_trials as needed\n",
    "    \n",
    "    print(\"\\n=== Optimization Complete ===\")\n",
    "    print(f\"Best parameters: {study.best_params}\")\n",
    "    print(f\"Best CV loss: {study.best_value:.6f}\")\n",
    "    \n",
    "    return study, X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "\n",
    "# ====================================================================\n",
    "# 10. EXAMPLE SINGLE TRIAL (FOR TESTING)\n",
    "# ====================================================================\n",
    "\n",
    "def test_single_trial():\n",
    "    \"\"\"Test a single trial with fixed parameters\"\"\"\n",
    "    print(\"=== Testing Single Trial ===\")\n",
    "    \n",
    "    # Load data\n",
    "    X, y = load_and_prepare_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, scaler = split_and_scale_data(X, y)\n",
    "    \n",
    "    # Fixed parameters for testing\n",
    "    model_params = {\n",
    "        'neurons': 16,\n",
    "        'dropout': 0.2\n",
    "    }\n",
    "    \n",
    "    adaptive_params = {\n",
    "        'initial_lr': 0.01,\n",
    "        'lr_min': 0.0001,\n",
    "        'initial_batch_size': 16,\n",
    "        'grad_threshold': 0.001,\n",
    "        'batch_multiplier': 2.0,\n",
    "        'lr_multiplier': 0.5,\n",
    "        'patience': 10\n",
    "    }\n",
    "    \n",
    "    # Setup MLflow\n",
    "    setup_mlflow(\"test_single_trial\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"single_test_trial\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params({**model_params, **adaptive_params})\n",
    "        \n",
    "        # Perform CV\n",
    "        cv_results = perform_kfold_cv(\n",
    "            X_train, y_train, \n",
    "            model_params, adaptive_params,\n",
    "            k_folds=4, max_epochs=50\n",
    "        )\n",
    "        \n",
    "        # Log results\n",
    "        log_cv_results_to_mlflow(cv_results)\n",
    "        \n",
    "        print(f\"Test trial CV loss: {cv_results['avg_cv_loss']:.6f}\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# ====================================================================\n",
    "# RUN THE CODE\n",
    "# ====================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment one of these to run:\n",
    "    \n",
    "    # For testing a single trial:\n",
    "    # test_results = test_single_trial()\n",
    "    \n",
    "    # For full optimization:\n",
    "    study, X_train, X_val, X_test, y_train, y_val, y_test, scaler = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
