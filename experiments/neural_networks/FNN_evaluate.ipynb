{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986ab7cc",
   "metadata": {},
   "source": [
    "**Feedforward Neural Network (FNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fc4c9",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebc28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a674b5",
   "metadata": {},
   "source": [
    "Classes / Functions used below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6960d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient-Based Adaptive Batch Size Class\n",
    "class GradientAdaptiveBatchingLearning:\n",
    "    def __init__(self, initial_batch_size=16, initial_lr=0.001, \n",
    "                 grad_threshold=0.01, batch_multiplier=1.4, lr_multiplier=0.8, lr_min=0.0005, max_batch_size=142):\n",
    "        self.current_batch_size = initial_batch_size\n",
    "        self.current_lr = initial_lr\n",
    "        self.grad_threshold = grad_threshold\n",
    "        self.batch_multiplier = batch_multiplier\n",
    "        self.lr_multiplier = lr_multiplier\n",
    "        self.lr_min = lr_min\n",
    "        self.max_batch_size = max_batch_size  # Will be set dynamically to len(X_train)\n",
    "        self.loss_history = []\n",
    "        self.update_count = 0\n",
    "        \n",
    "    def update_params(self, current_loss, epoch):\n",
    "        self.loss_history.append(current_loss)\n",
    "        \n",
    "        # Need at least 5 epochs of history for gradient calculation\n",
    "        if len(self.loss_history) >= 5:\n",
    "            # Calculate gradient (first derivative) over recent losses\n",
    "            recent_losses = self.loss_history[-5:]\n",
    "            gradient = np.mean(np.diff(recent_losses))\n",
    "            \n",
    "            # Additional stability check: don't update too frequently\n",
    "            epochs_since_update = len(self.loss_history) - self.update_count\n",
    "            \n",
    "            # If gradient is small (slow improvement) and enough time passed\n",
    "            if abs(gradient) < self.grad_threshold and epochs_since_update >= 10:\n",
    "                new_batch_size = min(\n",
    "                    int(self.current_batch_size * self.batch_multiplier), \n",
    "                    self.max_batch_size\n",
    "                )\n",
    "                # Decrease learning rate when increasing batch size\n",
    "                lr_scaling = self.lr_multiplier  # self.current_batch_size / new_batch_size\n",
    "                self.current_lr = max(\n",
    "                    self.current_lr*lr_scaling, \n",
    "                    self.lr_min\n",
    "                )\n",
    "                self.current_batch_size = new_batch_size\n",
    "                self.update_count = len(self.loss_history)\n",
    "                \n",
    "                print(f\"Epoch {epoch}: Gradient = {gradient:.6f} < {self.grad_threshold}\")\n",
    "                print(f\"  → Increased batch size: {int(self.current_batch_size/self.batch_multiplier)} → {self.current_batch_size}\")\n",
    "                print(f\"  → Decreased learning rate: {self.current_lr/lr_scaling:.6f} → {self.current_lr:.6f}\")\n",
    "\n",
    "                # if new_batch_size != self.current_batch_size:\n",
    "                #     # Decrease learning rate when increasing batch size\n",
    "                #     lr_scaling = self.lr_multiplier  # self.current_batch_size / new_batch_size\n",
    "                #     self.current_lr *= lr_scaling\n",
    "                #     self.current_batch_size = new_batch_size\n",
    "                #     self.update_count = len(self.loss_history)\n",
    "                    \n",
    "                #     print(f\"Epoch {epoch}: Gradient = {gradient:.6f} < {self.grad_threshold}\")\n",
    "                #     print(f\"  → Increased batch size: {int(self.current_batch_size/self.batch_multiplier)} → {self.current_batch_size}\")\n",
    "                #     print(f\"  → Decreased learning rate: {self.current_lr/lr_scaling:.6f} → {self.current_lr:.6f}\")\n",
    "                    \n",
    "        return self.current_batch_size, self.current_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b27da",
   "metadata": {},
   "source": [
    "Load csv into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f040e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\The Winner\\DSI\\customer_purchasing_behaviour\n",
      "Successfully loaded 'df_eng_customer_purchasing_features.csv' into the DataFrame named df.\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure consistent working directory for data loading ---\n",
    "# This block dynamically sets the current working directory to the Git repository root.\n",
    "# This makes data paths reliable for all collaborators, regardless of where they open the notebook.\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_root = current_dir\n",
    "while not os.path.exists(os.path.join(repo_root, '.git')):\n",
    "    # Move up one directory\n",
    "    parent_dir = os.path.dirname(repo_root)\n",
    "    if parent_dir == repo_root: # Reached filesystem root, .git not found\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the .git directory. \"\n",
    "            \"Please ensure you are running this code from within a Git repository.\"\n",
    "        )\n",
    "    repo_root = parent_dir\n",
    "\n",
    "# Change the current working directory if it's not already the repo root\n",
    "if os.getcwd() != repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\") # Informative print for users\n",
    "\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Path to the data file, relative to the repository root.\n",
    "data_file_name = 'df_eng_customer_purchasing_features.csv'\n",
    "data_file_path = os.path.join('data', 'processed', data_file_name)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    print(f\"Successfully loaded '{data_file_name}' into the DataFrame named df.\")\n",
    "    #print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{data_file_name}' was not found at '{data_file_path}'.\")\n",
    "    print(\"Please ensure it exists in the 'data/processed/' folder relative to the repository root.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15a58c",
   "metadata": {},
   "source": [
    "Split the data between train portion and a holdout portion that will be used in another Notebook for validation and test  \n",
    "*****************PENDING******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9696b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Hold out 20% of the dataset (to become val + test)\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(df.drop(['purchase_amount', 'log_purchase_amount'], axis=1), df[['purchase_amount', 'log_purchase_amount']], test_size=0.2, random_state=42)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "# Step 2: Split holdout into 10% validation and 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=42)\n",
    "df_val = pd.concat([X_val, y_val], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaad9b8",
   "metadata": {},
   "source": [
    "Prepare TRAIN data: scale numerical and encode categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9866895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.copy() \n",
    "\n",
    "# Preprocessing: Prepare Inputs\n",
    "\n",
    "# Extract relevant columns\n",
    "numerical_cols = ['age', 'annual_income', 'loyalty_score', 'purchase_frequency', 'spend_per_purchase', 'spend_to_income_ratio', 'log_annual_income', 'log_purchase_frequency']\n",
    "categorical_cols = ['region_grouped',  'is_high_value', 'is_champion']\n",
    "target_col = 'purchase_amount'\n",
    "target_col_log = 'log_purchase_amount'\n",
    "\n",
    "# Filter to only use columns that actually exist\n",
    "existing_numerical_cols = [col for col in numerical_cols if col in df.columns]\n",
    "existing_categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(df[existing_numerical_cols])\n",
    "\n",
    "# Encode categorical feature\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_cat = encoder.fit_transform(df[existing_categorical_cols])\n",
    "\n",
    "# Create DataFrames with distinct column names\n",
    "num_df = pd.DataFrame(X_num, columns=existing_numerical_cols)\n",
    "cat_columns = encoder.get_feature_names_out(existing_categorical_cols)\n",
    "cat_df = pd.DataFrame(X_cat, columns=cat_columns)\n",
    "\n",
    "# Combine safely\n",
    "combined_df = pd.concat([num_df, cat_df], axis=1)\n",
    "\n",
    "# Convert to torch tensor\n",
    "X_train = torch.tensor(combined_df.values, dtype=torch.float32)\n",
    "\n",
    "# Target variable\n",
    "y_train = torch.tensor(df[target_col].values, dtype=torch.float32).view(-1, 1)\n",
    "y_train_log = torch.tensor(df[target_col_log].values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e94e6",
   "metadata": {},
   "source": [
    "Prepare VAL data: scale numerical and encode categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_val.copy() \n",
    "\n",
    "# Preprocessing: Prepare Inputs\n",
    "\n",
    "# Extract relevant columns\n",
    "numerical_cols = ['age', 'annual_income', 'loyalty_score', 'purchase_frequency', 'spend_per_purchase', 'spend_to_income_ratio', 'log_annual_income', 'log_purchase_frequency']\n",
    "categorical_cols = ['region_grouped',  'is_high_value', 'is_champion']\n",
    "target_col = 'purchase_amount'\n",
    "target_col_log = 'log_purchase_amount'\n",
    "\n",
    "# Filter to only use columns that actually exist\n",
    "existing_numerical_cols = [col for col in numerical_cols if col in df.columns]\n",
    "existing_categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "# Scale numerical features\n",
    "#scaler = StandardScaler()\n",
    "X_num = scaler.transform(df[existing_numerical_cols])\n",
    "\n",
    "# Encode categorical feature\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_cat = encoder.fit_transform(df[existing_categorical_cols])\n",
    "\n",
    "# Create DataFrames with distinct column names\n",
    "num_df = pd.DataFrame(X_num, columns=existing_numerical_cols)\n",
    "cat_columns = encoder.get_feature_names_out(existing_categorical_cols)\n",
    "cat_df = pd.DataFrame(X_cat, columns=cat_columns)\n",
    "\n",
    "# Combine safely\n",
    "combined_df = pd.concat([num_df, cat_df], axis=1)\n",
    "\n",
    "# Convert to torch tensor\n",
    "X_val = torch.tensor(combined_df.values, dtype=torch.float32)\n",
    "\n",
    "# Target variable\n",
    "y_val = torch.tensor(df[target_col].values, dtype=torch.float32).view(-1, 1)\n",
    "y_val_log = torch.tensor(df[target_col_log].values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5a34c",
   "metadata": {},
   "source": [
    "Prepare TEST data: scale numerical and encode categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbaa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_test.copy() \n",
    "\n",
    "# Preprocessing: Prepare Inputs\n",
    "\n",
    "# Extract relevant columns\n",
    "numerical_cols = ['age', 'annual_income', 'loyalty_score', 'purchase_frequency', 'spend_per_purchase', 'spend_to_income_ratio', 'log_annual_income', 'log_purchase_frequency']\n",
    "categorical_cols = ['region_grouped',  'is_high_value', 'is_champion']\n",
    "target_col = 'purchase_amount'\n",
    "target_col_log = 'log_purchase_amount'\n",
    "\n",
    "# Filter to only use columns that actually exist\n",
    "existing_numerical_cols = [col for col in numerical_cols if col in df.columns]\n",
    "existing_categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "# Scale numerical features\n",
    "#scaler = StandardScaler()\n",
    "X_num = scaler.transform(df[existing_numerical_cols])\n",
    "\n",
    "# Encode categorical feature\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_cat = encoder.fit_transform(df[existing_categorical_cols])\n",
    "\n",
    "# Create DataFrames with distinct column names\n",
    "num_df = pd.DataFrame(X_num, columns=existing_numerical_cols)\n",
    "cat_columns = encoder.get_feature_names_out(existing_categorical_cols)\n",
    "cat_df = pd.DataFrame(X_cat, columns=cat_columns)\n",
    "\n",
    "# Combine safely\n",
    "combined_df = pd.concat([num_df, cat_df], axis=1)\n",
    "\n",
    "# Convert to torch tensor\n",
    "X_test = torch.tensor(combined_df.values, dtype=torch.float32)\n",
    "\n",
    "# Target variable\n",
    "y_test = torch.tensor(df[target_col].values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_log = torch.tensor(df[target_col_log].values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d7b4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "  Train: (190, 190, torch.Size([190, 15]), torch.Size([190, 1])) (will be max batch size)\n",
      "  Validation: (24, 24, torch.Size([24, 15]), torch.Size([24, 1]))\n",
      "  Test: (24, 24, torch.Size([24, 15]), torch.Size([24, 1]))\n"
     ]
    }
   ],
   "source": [
    "# Print dataset sizes and set max batch sizes\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(X_train), len(y_train), X_train.shape, y_train.shape} (will be max batch size)\")\n",
    "print(f\"  Validation: {len(X_val), len(y_val), X_val.shape, y_val.shape}\")\n",
    "print(f\"  Test: {len(X_test), len(y_test), X_test.shape, y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b52f03",
   "metadata": {},
   "source": [
    "Run Model and log it in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e702375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dotenv() returned: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e5635dff034970a37a683fbad48a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Train → MSE = 199697.1406, RMSE = 446.8748, MAE = 425.2408, R² = -9.6480\n",
      "Val   → MSE = 187224.7500, RMSE = 432.6948, MAE = 401.0015, R² = -6.1077\n",
      "Epoch 225: Gradient = 0.027420 < 0.05\n",
      "  → Increased batch size: 16 → 32\n",
      "  → Decreased learning rate: 0.005000 → 0.002500\n",
      "Epoch 236: Gradient = -0.034897 < 0.05\n",
      "  → Increased batch size: 32 → 64\n",
      "  → Decreased learning rate: 0.002500 → 0.001250\n",
      "Epoch 257: Gradient = -0.036270 < 0.05\n",
      "  → Increased batch size: 64 → 128\n",
      "  → Decreased learning rate: 0.001250 → 0.000625\n",
      "Epoch 267: Gradient = 0.002563 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 277: Gradient = -0.039474 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 287: Gradient = -0.011032 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 297: Gradient = -0.002380 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 307: Gradient = 0.007446 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 317: Gradient = -0.002686 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 327: Gradient = -0.005539 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 337: Gradient = -0.000061 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 347: Gradient = 0.000229 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 357: Gradient = -0.001617 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 367: Gradient = -0.002670 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 377: Gradient = -0.001831 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 387: Gradient = -0.000305 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 397: Gradient = -0.001221 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 407: Gradient = -0.001221 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 417: Gradient = -0.002548 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 427: Gradient = -0.001862 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 437: Gradient = -0.001266 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 447: Gradient = -0.002304 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 457: Gradient = -0.001602 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 467: Gradient = -0.001389 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 477: Gradient = -0.001465 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 487: Gradient = -0.000427 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 497: Gradient = -0.001602 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 507: Gradient = -0.002609 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 517: Gradient = -0.000931 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 527: Gradient = -0.002136 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 537: Gradient = -0.001038 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 547: Gradient = -0.002960 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 557: Gradient = -0.002197 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 567: Gradient = -0.002899 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 577: Gradient = -0.000610 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 587: Gradient = -0.000717 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 597: Gradient = -0.000595 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 607: Gradient = -0.002090 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 617: Gradient = -0.002197 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 627: Gradient = -0.003128 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 637: Gradient = -0.002167 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 647: Gradient = -0.002487 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 657: Gradient = -0.001022 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 667: Gradient = -0.002518 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 677: Gradient = -0.000717 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 687: Gradient = -0.001907 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 697: Gradient = -0.001160 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 707: Gradient = -0.002319 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 717: Gradient = -0.001266 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 727: Gradient = -0.002121 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 737: Gradient = -0.001282 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 747: Gradient = -0.002228 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 757: Gradient = -0.004105 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 767: Gradient = -0.001709 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 777: Gradient = -0.001846 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 787: Gradient = -0.000916 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 797: Gradient = -0.002548 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 807: Gradient = -0.003128 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 817: Gradient = -0.001404 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 827: Gradient = -0.002899 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 837: Gradient = -0.001190 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 847: Gradient = -0.003067 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 857: Gradient = -0.002090 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 867: Gradient = -0.003677 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 877: Gradient = -0.002075 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 887: Gradient = -0.002136 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 897: Gradient = -0.002014 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 907: Gradient = -0.002228 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 917: Gradient = -0.001495 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 927: Gradient = -0.002792 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 937: Gradient = -0.003052 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 947: Gradient = -0.002579 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 957: Gradient = -0.003830 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 967: Gradient = -0.002136 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 977: Gradient = -0.001572 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 987: Gradient = -0.001740 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 997: Gradient = -0.002808 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1007: Gradient = -0.001648 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1017: Gradient = -0.002380 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1027: Gradient = -0.002472 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1037: Gradient = -0.002365 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1047: Gradient = -0.002258 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1057: Gradient = -0.001038 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1067: Gradient = -0.002151 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1077: Gradient = -0.002808 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1087: Gradient = -0.002686 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1097: Gradient = -0.000229 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1107: Gradient = -0.004639 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1117: Gradient = -0.000366 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1127: Gradient = -0.004105 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1137: Gradient = -0.001038 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1147: Gradient = -0.001709 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1157: Gradient = 0.000549 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1167: Gradient = -0.000580 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1177: Gradient = 0.000809 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1187: Gradient = -0.000488 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1197: Gradient = 0.000153 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1207: Gradient = -0.004120 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1217: Gradient = -0.004974 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1227: Gradient = -0.005280 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1237: Gradient = -0.005386 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1247: Gradient = -0.004578 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1257: Gradient = -0.005157 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1267: Gradient = -0.002640 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1277: Gradient = -0.001221 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1287: Gradient = 0.001358 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1297: Gradient = -0.001648 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1307: Gradient = -0.002747 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1317: Gradient = -0.001663 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1327: Gradient = -0.003494 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1337: Gradient = -0.002487 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1347: Gradient = -0.000702 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1357: Gradient = -0.003525 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1367: Gradient = -0.002075 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1377: Gradient = -0.003372 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1387: Gradient = -0.003265 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1397: Gradient = -0.000900 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1407: Gradient = -0.002319 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1417: Gradient = -0.001282 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1427: Gradient = -0.004379 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1437: Gradient = -0.002472 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1447: Gradient = -0.002975 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1457: Gradient = -0.002426 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1467: Gradient = -0.002441 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1477: Gradient = -0.003006 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1487: Gradient = -0.004547 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1497: Gradient = -0.001495 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1507: Gradient = -0.003540 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1517: Gradient = 0.000046 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1527: Gradient = -0.003754 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1537: Gradient = -0.002914 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1547: Gradient = -0.003296 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1557: Gradient = -0.002625 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1567: Gradient = -0.000504 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1577: Gradient = -0.004333 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1587: Gradient = -0.001495 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1597: Gradient = -0.002396 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1607: Gradient = -0.001999 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1617: Gradient = -0.001465 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1627: Gradient = -0.004730 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1637: Gradient = -0.005890 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1647: Gradient = -0.002731 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1657: Gradient = -0.001740 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1667: Gradient = -0.002075 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1677: Gradient = -0.002914 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1687: Gradient = -0.000702 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1697: Gradient = -0.003708 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1707: Gradient = -0.002548 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1717: Gradient = -0.004868 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1727: Gradient = -0.002060 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1737: Gradient = -0.000443 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1747: Gradient = -0.003952 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1757: Gradient = -0.003159 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1767: Gradient = -0.003296 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1777: Gradient = -0.001694 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1787: Gradient = -0.001968 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1797: Gradient = -0.003693 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1807: Gradient = -0.007095 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1817: Gradient = -0.003937 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1827: Gradient = -0.000626 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1837: Gradient = -0.003052 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1847: Gradient = -0.007385 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1857: Gradient = -0.007523 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1867: Gradient = -0.004120 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1877: Gradient = -0.002396 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1887: Gradient = -0.000778 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1897: Gradient = -0.002380 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1907: Gradient = -0.003174 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1917: Gradient = -0.004929 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1927: Gradient = -0.004929 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1937: Gradient = -0.004593 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1947: Gradient = -0.004135 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1957: Gradient = -0.007767 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1967: Gradient = -0.004410 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1977: Gradient = -0.001770 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1987: Gradient = -0.004608 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 1997: Gradient = -0.000122 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2007: Gradient = -0.006073 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2017: Gradient = -0.004135 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2027: Gradient = -0.002625 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2037: Gradient = -0.003784 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2047: Gradient = -0.000687 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2057: Gradient = -0.006653 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2067: Gradient = -0.004166 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2077: Gradient = -0.003647 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2087: Gradient = -0.002777 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2097: Gradient = -0.008469 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2107: Gradient = -0.003601 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2117: Gradient = -0.007080 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2127: Gradient = -0.000702 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2137: Gradient = -0.003967 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2147: Gradient = -0.001251 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2157: Gradient = -0.004868 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2167: Gradient = -0.003906 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2177: Gradient = -0.004715 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2187: Gradient = -0.000565 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2197: Gradient = -0.004471 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2207: Gradient = -0.011017 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2217: Gradient = -0.005249 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2227: Gradient = 0.000946 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2237: Gradient = 0.001038 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2247: Gradient = -0.004547 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2257: Gradient = -0.003052 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2267: Gradient = -0.006561 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2277: Gradient = -0.001633 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2287: Gradient = -0.002762 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2297: Gradient = -0.003540 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2307: Gradient = -0.005859 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2317: Gradient = -0.000015 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2327: Gradient = 0.002289 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2337: Gradient = -0.005814 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2347: Gradient = -0.000076 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2357: Gradient = 0.000610 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2367: Gradient = -0.005524 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2377: Gradient = -0.002106 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2387: Gradient = -0.004745 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2397: Gradient = -0.006287 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2407: Gradient = -0.005630 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2417: Gradient = -0.009674 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2427: Gradient = -0.006302 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2437: Gradient = -0.009201 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2447: Gradient = -0.003189 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2457: Gradient = -0.005203 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2467: Gradient = -0.002167 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2477: Gradient = -0.001389 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2487: Gradient = -0.004211 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2497: Gradient = -0.007141 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2507: Gradient = -0.010834 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2517: Gradient = -0.008179 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2527: Gradient = -0.005997 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2537: Gradient = -0.002075 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2547: Gradient = -0.001343 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2557: Gradient = -0.004761 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2567: Gradient = -0.000412 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2577: Gradient = 0.001297 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2587: Gradient = -0.006912 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2597: Gradient = -0.003448 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2607: Gradient = -0.006287 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2617: Gradient = -0.001083 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2627: Gradient = 0.001678 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2637: Gradient = 0.002274 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2647: Gradient = 0.000732 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2657: Gradient = -0.001373 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2667: Gradient = -0.004517 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2677: Gradient = -0.006195 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2687: Gradient = -0.007965 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2697: Gradient = -0.014648 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2707: Gradient = -0.011185 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2717: Gradient = -0.004150 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2727: Gradient = 0.000580 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2737: Gradient = 0.001953 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2747: Gradient = -0.000534 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2757: Gradient = -0.000412 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2767: Gradient = 0.000732 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2777: Gradient = -0.002518 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2787: Gradient = -0.009384 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2797: Gradient = -0.010696 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2807: Gradient = -0.011703 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2817: Gradient = -0.006210 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2827: Gradient = -0.002228 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2837: Gradient = -0.003693 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2847: Gradient = 0.005066 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2857: Gradient = -0.000504 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2867: Gradient = 0.003128 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2877: Gradient = -0.003494 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2887: Gradient = -0.006546 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2897: Gradient = -0.007156 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2907: Gradient = -0.005722 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2917: Gradient = 0.002930 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2927: Gradient = -0.004883 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2937: Gradient = -0.002808 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2947: Gradient = 0.001007 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2957: Gradient = -0.005051 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2967: Gradient = -0.007202 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2977: Gradient = -0.007889 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2987: Gradient = 0.002075 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 2997: Gradient = -0.000397 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3007: Gradient = -0.004929 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3017: Gradient = -0.001541 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3027: Gradient = 0.001648 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3037: Gradient = -0.004944 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3047: Gradient = -0.010864 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3057: Gradient = -0.000336 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3067: Gradient = 0.001160 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3077: Gradient = -0.009491 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3087: Gradient = -0.001022 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3097: Gradient = -0.009247 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3107: Gradient = -0.005295 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3117: Gradient = -0.002945 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3127: Gradient = -0.018188 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3137: Gradient = -0.012558 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3147: Gradient = -0.005615 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3157: Gradient = -0.001724 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3167: Gradient = -0.005753 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3177: Gradient = -0.000122 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3187: Gradient = -0.003082 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3197: Gradient = -0.007202 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3207: Gradient = -0.003571 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3217: Gradient = -0.003540 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3227: Gradient = -0.006241 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3237: Gradient = -0.003174 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3247: Gradient = -0.005127 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3257: Gradient = -0.005447 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3267: Gradient = -0.009857 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3277: Gradient = -0.018372 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3287: Gradient = -0.006424 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3297: Gradient = -0.004272 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3307: Gradient = -0.002731 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3317: Gradient = -0.005020 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3327: Gradient = -0.006348 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3337: Gradient = -0.002243 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3347: Gradient = -0.008362 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3357: Gradient = 0.000961 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3367: Gradient = -0.003464 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3377: Gradient = -0.011124 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3387: Gradient = -0.003311 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3397: Gradient = -0.007812 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3407: Gradient = -0.007355 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3417: Gradient = 0.001816 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3427: Gradient = -0.006149 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3437: Gradient = -0.003296 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3447: Gradient = -0.006302 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3457: Gradient = -0.010712 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3467: Gradient = -0.002441 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3477: Gradient = -0.005310 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3487: Gradient = -0.005127 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3497: Gradient = 0.002640 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3507: Gradient = -0.007568 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3517: Gradient = -0.007599 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3527: Gradient = -0.000061 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3537: Gradient = -0.003983 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3547: Gradient = -0.014191 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3557: Gradient = -0.016479 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3567: Gradient = -0.015808 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3577: Gradient = -0.001022 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3587: Gradient = -0.000809 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3597: Gradient = -0.007172 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3607: Gradient = -0.005280 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3617: Gradient = 0.011902 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3627: Gradient = 0.003662 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3637: Gradient = -0.011780 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3647: Gradient = -0.014160 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3657: Gradient = -0.017349 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3667: Gradient = -0.010056 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3677: Gradient = 0.002075 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3687: Gradient = -0.006012 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3697: Gradient = -0.011475 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3707: Gradient = -0.013412 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3717: Gradient = -0.009277 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3727: Gradient = -0.003036 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3737: Gradient = -0.015869 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3747: Gradient = -0.009445 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3757: Gradient = 0.009354 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3767: Gradient = 0.012970 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3777: Gradient = -0.005524 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3787: Gradient = -0.015686 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3797: Gradient = -0.004883 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3807: Gradient = 0.003159 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3817: Gradient = -0.011337 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3827: Gradient = -0.018951 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3837: Gradient = 0.003220 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3847: Gradient = 0.008911 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3857: Gradient = -0.004471 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3867: Gradient = -0.012512 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3877: Gradient = -0.006836 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3887: Gradient = -0.005676 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3897: Gradient = -0.004517 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3907: Gradient = -0.004944 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3917: Gradient = -0.013290 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3927: Gradient = -0.006287 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3937: Gradient = 0.002136 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3947: Gradient = -0.003174 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3957: Gradient = -0.009521 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3967: Gradient = -0.008301 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3977: Gradient = 0.003647 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3987: Gradient = -0.005859 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 3997: Gradient = -0.013351 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4007: Gradient = -0.009048 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4017: Gradient = -0.002380 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4027: Gradient = 0.004379 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4037: Gradient = 0.002441 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4047: Gradient = -0.013550 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4057: Gradient = -0.015701 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4067: Gradient = -0.004028 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4077: Gradient = 0.002365 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4087: Gradient = -0.004959 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4097: Gradient = -0.007690 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4107: Gradient = -0.005615 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4117: Gradient = -0.007690 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4127: Gradient = -0.012009 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4137: Gradient = -0.007477 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4147: Gradient = 0.002380 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4157: Gradient = 0.003784 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4167: Gradient = -0.009354 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4177: Gradient = -0.013855 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4187: Gradient = 0.001068 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4197: Gradient = 0.001129 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4207: Gradient = -0.009705 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4217: Gradient = -0.011353 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4227: Gradient = -0.005798 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4237: Gradient = -0.002701 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4247: Gradient = -0.006866 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4257: Gradient = 0.002869 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4267: Gradient = 0.013733 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4277: Gradient = 0.008286 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4287: Gradient = -0.003510 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4297: Gradient = -0.018158 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4307: Gradient = -0.019379 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4317: Gradient = -0.008652 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4327: Gradient = 0.001556 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4337: Gradient = -0.003418 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4347: Gradient = -0.019394 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4357: Gradient = -0.011719 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4367: Gradient = 0.005814 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4377: Gradient = 0.023102 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4387: Gradient = 0.014175 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4397: Gradient = -0.010117 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4407: Gradient = -0.010544 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4417: Gradient = -0.002869 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4427: Gradient = -0.005798 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4437: Gradient = -0.004837 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4447: Gradient = -0.001236 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4457: Gradient = 0.001709 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4467: Gradient = -0.005951 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4477: Gradient = -0.019165 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4487: Gradient = -0.022293 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4497: Gradient = -0.005203 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4507: Gradient = -0.002090 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4517: Gradient = -0.006683 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4527: Gradient = -0.012390 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4537: Gradient = -0.006836 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4547: Gradient = 0.005432 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4557: Gradient = 0.002457 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4567: Gradient = -0.003082 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4577: Gradient = -0.003113 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4587: Gradient = 0.011673 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4597: Gradient = 0.019562 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4607: Gradient = 0.009979 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4617: Gradient = 0.001602 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4627: Gradient = -0.000320 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4637: Gradient = 0.004150 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4647: Gradient = 0.003967 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4657: Gradient = 0.001099 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4667: Gradient = 0.003906 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4677: Gradient = -0.001389 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4687: Gradient = 0.000259 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4697: Gradient = -0.001190 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4707: Gradient = -0.014816 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4717: Gradient = -0.024628 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4727: Gradient = -0.021637 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4737: Gradient = -0.011795 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4747: Gradient = 0.005051 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4757: Gradient = 0.003250 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4767: Gradient = -0.011963 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4777: Gradient = -0.020142 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4787: Gradient = -0.011108 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4797: Gradient = -0.011551 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4807: Gradient = -0.006897 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4817: Gradient = 0.005646 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4827: Gradient = 0.012085 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4837: Gradient = 0.001465 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4847: Gradient = -0.017334 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4857: Gradient = -0.022629 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4867: Gradient = -0.015198 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4877: Gradient = -0.007965 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4887: Gradient = -0.001175 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4897: Gradient = -0.009521 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4907: Gradient = -0.017944 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4917: Gradient = -0.010330 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4927: Gradient = 0.004868 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4937: Gradient = 0.011002 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4947: Gradient = 0.002457 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4957: Gradient = -0.014603 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4967: Gradient = -0.023682 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4977: Gradient = -0.014008 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4987: Gradient = 0.001938 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 4997: Gradient = 0.008499 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5007: Gradient = -0.005127 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5017: Gradient = -0.019318 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5027: Gradient = -0.015915 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5037: Gradient = 0.003601 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5047: Gradient = 0.019241 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5057: Gradient = 0.012024 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5067: Gradient = -0.013992 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5077: Gradient = -0.027466 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5087: Gradient = -0.010223 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5097: Gradient = 0.007233 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5107: Gradient = 0.008453 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5117: Gradient = -0.004684 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5127: Gradient = -0.020859 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5137: Gradient = -0.018555 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5147: Gradient = -0.017776 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5157: Gradient = -0.004639 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5167: Gradient = 0.003891 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5177: Gradient = -0.007294 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5187: Gradient = -0.022720 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5197: Gradient = -0.013794 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5207: Gradient = -0.017776 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5217: Gradient = -0.030380 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5227: Gradient = -0.035690 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5237: Gradient = -0.013092 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5247: Gradient = 0.012482 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5257: Gradient = 0.015411 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5267: Gradient = 0.001602 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5277: Gradient = -0.003021 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5287: Gradient = -0.006912 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5297: Gradient = -0.009506 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5307: Gradient = 0.009567 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5317: Gradient = 0.018433 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5327: Gradient = 0.002975 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5337: Gradient = -0.010086 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5347: Gradient = -0.007782 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5357: Gradient = -0.008987 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5367: Gradient = -0.006958 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5377: Gradient = -0.003799 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5387: Gradient = -0.006836 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5397: Gradient = -0.009415 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5407: Gradient = -0.006607 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5417: Gradient = -0.005554 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5427: Gradient = -0.005203 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5437: Gradient = -0.004242 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5447: Gradient = -0.004807 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5457: Gradient = -0.005905 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5467: Gradient = -0.016281 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5477: Gradient = -0.022186 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5487: Gradient = -0.014954 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5497: Gradient = 0.016846 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5507: Gradient = 0.023132 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5517: Gradient = -0.001053 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5527: Gradient = -0.014496 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5537: Gradient = -0.030563 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5547: Gradient = -0.028320 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5557: Gradient = -0.000122 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5567: Gradient = 0.019363 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5577: Gradient = 0.009018 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5587: Gradient = -0.005859 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5597: Gradient = -0.033203 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5607: Gradient = -0.032532 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5617: Gradient = -0.012238 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5627: Gradient = 0.015427 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5637: Gradient = 0.016296 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5647: Gradient = 0.002808 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5657: Gradient = -0.018570 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5667: Gradient = -0.020691 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5677: Gradient = -0.003494 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5687: Gradient = 0.003174 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5697: Gradient = -0.013062 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5707: Gradient = -0.024796 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5717: Gradient = -0.013855 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5727: Gradient = 0.008453 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5737: Gradient = -0.000397 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5747: Gradient = -0.012375 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5757: Gradient = -0.003036 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5767: Gradient = -0.019730 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5777: Gradient = -0.015091 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5787: Gradient = 0.000351 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5797: Gradient = 0.007843 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5807: Gradient = 0.014999 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5817: Gradient = -0.004349 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5827: Gradient = -0.023727 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5837: Gradient = -0.025696 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5847: Gradient = -0.003311 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5857: Gradient = 0.004684 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5867: Gradient = 0.003418 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5877: Gradient = 0.010742 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5887: Gradient = -0.025070 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5897: Gradient = -0.016998 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5907: Gradient = -0.005646 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5917: Gradient = 0.002686 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5927: Gradient = -0.006607 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5937: Gradient = 0.007950 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5947: Gradient = 0.004761 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5957: Gradient = -0.012466 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5967: Gradient = -0.020554 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5977: Gradient = -0.018509 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5987: Gradient = 0.008087 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 5997: Gradient = 0.011642 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6007: Gradient = 0.011154 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6017: Gradient = -0.008179 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6027: Gradient = -0.012146 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6037: Gradient = -0.011948 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6047: Gradient = -0.004517 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6057: Gradient = 0.001251 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6067: Gradient = -0.009918 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6077: Gradient = -0.004349 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6087: Gradient = -0.006134 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6097: Gradient = -0.003922 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6107: Gradient = -0.007614 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6117: Gradient = 0.006592 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6127: Gradient = -0.021851 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6137: Gradient = 0.006653 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6147: Gradient = -0.010880 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6157: Gradient = 0.005676 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6167: Gradient = -0.004944 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6177: Gradient = 0.001511 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6187: Gradient = -0.019196 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6197: Gradient = -0.012817 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6207: Gradient = -0.004120 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6217: Gradient = 0.005829 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6227: Gradient = 0.006668 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6237: Gradient = -0.002106 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6247: Gradient = -0.012817 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6257: Gradient = -0.012817 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6267: Gradient = -0.006516 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6277: Gradient = 0.000626 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6287: Gradient = 0.010513 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6297: Gradient = -0.005966 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6307: Gradient = -0.007187 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6317: Gradient = -0.004837 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6327: Gradient = -0.002853 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6337: Gradient = -0.008743 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6347: Gradient = 0.010101 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6357: Gradient = -0.010620 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6367: Gradient = -0.003052 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6377: Gradient = 0.003906 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6387: Gradient = -0.027100 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6397: Gradient = 0.016037 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6407: Gradient = -0.005508 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6417: Gradient = 0.006470 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6427: Gradient = -0.004395 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6437: Gradient = -0.000046 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6447: Gradient = -0.010208 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6457: Gradient = -0.000336 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6467: Gradient = 0.001633 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6477: Gradient = -0.000778 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6487: Gradient = -0.006119 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6497: Gradient = -0.002991 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6507: Gradient = 0.001877 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6517: Gradient = -0.004257 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6527: Gradient = 0.004425 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6537: Gradient = -0.000107 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6547: Gradient = 0.005905 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6557: Gradient = -0.019455 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6567: Gradient = 0.014648 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6577: Gradient = 0.001358 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6587: Gradient = -0.001328 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6597: Gradient = -0.008560 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6607: Gradient = 0.008087 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6617: Gradient = 0.006592 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6627: Gradient = -0.015594 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6637: Gradient = 0.006409 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6647: Gradient = -0.001114 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6657: Gradient = 0.013336 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6667: Gradient = -0.012085 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6677: Gradient = -0.004364 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6687: Gradient = 0.007034 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6697: Gradient = 0.006851 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6707: Gradient = -0.006775 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6717: Gradient = -0.011841 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6727: Gradient = 0.007507 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6737: Gradient = 0.002640 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6747: Gradient = -0.012512 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6757: Gradient = 0.001205 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6767: Gradient = 0.002716 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6777: Gradient = -0.014023 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6787: Gradient = -0.001587 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6797: Gradient = 0.007782 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6807: Gradient = 0.002655 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6817: Gradient = -0.000870 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6827: Gradient = -0.006104 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6837: Gradient = 0.003815 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6847: Gradient = 0.005249 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6857: Gradient = -0.007874 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6867: Gradient = -0.002579 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6877: Gradient = -0.002594 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6887: Gradient = 0.000000 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6897: Gradient = -0.000488 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6907: Gradient = 0.005249 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6917: Gradient = 0.004990 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6927: Gradient = -0.019852 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6937: Gradient = -0.006577 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6947: Gradient = 0.013489 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6957: Gradient = 0.000931 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6967: Gradient = -0.000595 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6977: Gradient = -0.010880 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6987: Gradient = 0.000305 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 6997: Gradient = 0.000687 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7007: Gradient = -0.003586 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7017: Gradient = 0.000961 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7027: Gradient = -0.010513 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7037: Gradient = 0.004669 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7047: Gradient = -0.006500 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7057: Gradient = -0.001831 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7067: Gradient = -0.002151 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7077: Gradient = -0.006256 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7087: Gradient = 0.006470 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7097: Gradient = 0.004211 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7107: Gradient = 0.012650 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7117: Gradient = -0.003983 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7127: Gradient = 0.000671 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7137: Gradient = 0.003510 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7147: Gradient = 0.009888 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7157: Gradient = -0.006973 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7167: Gradient = 0.005188 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7177: Gradient = 0.011475 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7187: Gradient = -0.009567 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7197: Gradient = -0.001633 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7207: Gradient = -0.014893 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7217: Gradient = -0.003738 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7227: Gradient = 0.000168 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7237: Gradient = 0.009140 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7247: Gradient = -0.011597 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7257: Gradient = 0.000107 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7267: Gradient = 0.008362 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7277: Gradient = 0.001801 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7287: Gradient = -0.001022 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7297: Gradient = -0.019714 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7307: Gradient = -0.008392 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7317: Gradient = -0.014999 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7327: Gradient = 0.002197 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7337: Gradient = 0.005508 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7347: Gradient = 0.004639 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7357: Gradient = -0.012817 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7367: Gradient = -0.010986 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7377: Gradient = -0.002991 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7387: Gradient = -0.006027 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7397: Gradient = -0.010300 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7407: Gradient = -0.003983 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7417: Gradient = 0.001984 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7427: Gradient = 0.001999 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7437: Gradient = 0.013199 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7447: Gradient = 0.016190 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7457: Gradient = 0.011734 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7467: Gradient = -0.004501 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7477: Gradient = 0.007874 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7487: Gradient = 0.003815 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7497: Gradient = -0.005493 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7507: Gradient = -0.002930 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7517: Gradient = -0.001877 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7527: Gradient = 0.000031 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7537: Gradient = -0.003754 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7547: Gradient = -0.001709 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7557: Gradient = -0.000046 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7567: Gradient = -0.001328 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7577: Gradient = 0.008621 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7587: Gradient = -0.000183 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7597: Gradient = -0.000397 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7607: Gradient = 0.000168 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7617: Gradient = 0.000687 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7627: Gradient = 0.000534 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7637: Gradient = -0.008438 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7647: Gradient = -0.001923 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7657: Gradient = -0.001755 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7667: Gradient = 0.000336 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7677: Gradient = 0.003387 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7687: Gradient = -0.005905 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7697: Gradient = -0.010223 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7707: Gradient = -0.007339 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7717: Gradient = -0.004883 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7727: Gradient = -0.004471 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7737: Gradient = 0.003830 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7747: Gradient = 0.001221 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7757: Gradient = 0.002670 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7767: Gradient = -0.002258 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7777: Gradient = 0.001907 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7787: Gradient = 0.004440 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7797: Gradient = 0.001587 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7807: Gradient = 0.003555 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7817: Gradient = -0.001938 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7827: Gradient = 0.002441 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7837: Gradient = 0.004608 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7847: Gradient = 0.009659 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7857: Gradient = -0.000198 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7867: Gradient = -0.000107 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7877: Gradient = 0.011093 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7887: Gradient = 0.011795 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7897: Gradient = 0.004257 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7907: Gradient = 0.000320 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7917: Gradient = -0.002808 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7927: Gradient = 0.003540 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7937: Gradient = 0.005692 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7947: Gradient = 0.001770 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7957: Gradient = -0.002808 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7967: Gradient = 0.002380 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7977: Gradient = -0.004196 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7987: Gradient = 0.002899 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 7997: Gradient = 0.001816 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8007: Gradient = -0.000336 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8017: Gradient = -0.003281 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8027: Gradient = 0.001221 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8037: Gradient = 0.004639 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8047: Gradient = 0.000305 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8057: Gradient = -0.000931 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8067: Gradient = 0.000534 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8077: Gradient = -0.003235 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8087: Gradient = -0.000381 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8097: Gradient = 0.003677 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8107: Gradient = 0.000336 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8117: Gradient = 0.001328 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8127: Gradient = 0.002319 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8137: Gradient = 0.002502 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8147: Gradient = 0.004227 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8157: Gradient = 0.000000 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8167: Gradient = 0.000610 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8177: Gradient = 0.003738 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8187: Gradient = -0.002151 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8197: Gradient = 0.000397 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8207: Gradient = -0.000885 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8217: Gradient = -0.000870 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8227: Gradient = 0.000778 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8237: Gradient = 0.000381 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8247: Gradient = 0.004837 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8257: Gradient = 0.000977 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8267: Gradient = 0.001404 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8277: Gradient = -0.000565 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8287: Gradient = 0.002396 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8297: Gradient = 0.003784 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8307: Gradient = -0.000366 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8317: Gradient = -0.001129 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8327: Gradient = -0.001495 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8337: Gradient = 0.003098 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8347: Gradient = 0.004349 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8357: Gradient = -0.003265 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8367: Gradient = 0.000854 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8377: Gradient = 0.000778 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8387: Gradient = 0.003418 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8397: Gradient = 0.000381 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8407: Gradient = 0.000809 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8417: Gradient = 0.002441 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8427: Gradient = 0.004272 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8437: Gradient = 0.001373 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8447: Gradient = -0.000626 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8457: Gradient = 0.003265 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8467: Gradient = 0.004166 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8477: Gradient = 0.004257 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8487: Gradient = -0.001678 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8497: Gradient = 0.000397 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8507: Gradient = -0.002991 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8517: Gradient = 0.002762 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8527: Gradient = 0.004807 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8537: Gradient = 0.000366 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8547: Gradient = 0.001389 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8557: Gradient = 0.004807 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8567: Gradient = 0.006729 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8577: Gradient = 0.002808 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8587: Gradient = 0.005096 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8597: Gradient = -0.001892 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8607: Gradient = 0.004440 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8617: Gradient = 0.007538 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8627: Gradient = 0.000473 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8637: Gradient = 0.004120 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8647: Gradient = 0.005615 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8657: Gradient = 0.002533 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8667: Gradient = 0.001404 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8677: Gradient = 0.001266 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8687: Gradient = -0.001495 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8697: Gradient = -0.000061 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8707: Gradient = 0.003815 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8717: Gradient = -0.002899 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8727: Gradient = -0.002609 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8737: Gradient = 0.000107 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8747: Gradient = 0.007843 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8757: Gradient = 0.002014 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8767: Gradient = 0.001129 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8777: Gradient = 0.003922 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8787: Gradient = 0.000305 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8797: Gradient = -0.001709 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8807: Gradient = 0.006454 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8817: Gradient = 0.003967 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8827: Gradient = -0.001022 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8837: Gradient = -0.003799 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8847: Gradient = -0.006912 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8857: Gradient = 0.005142 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8867: Gradient = 0.004410 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8877: Gradient = -0.000198 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8887: Gradient = 0.008011 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8897: Gradient = 0.007065 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8907: Gradient = -0.003174 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8917: Gradient = 0.000992 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8927: Gradient = -0.000336 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8937: Gradient = -0.008911 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8965: Gradient = 0.041870 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 8982: Gradient = -0.009399 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9006: Gradient = 0.033630 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9023: Gradient = 0.024017 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9033: Gradient = -0.015991 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9043: Gradient = 0.016800 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9053: Gradient = -0.015350 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9063: Gradient = 0.010193 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9073: Gradient = 0.005402 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9083: Gradient = -0.000565 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9093: Gradient = -0.003403 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9103: Gradient = -0.018845 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9133: Gradient = -0.035034 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9153: Gradient = -0.021851 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9174: Gradient = 0.024765 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9188: Gradient = -0.022995 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9198: Gradient = -0.025253 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9208: Gradient = 0.036072 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9218: Gradient = -0.014252 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9228: Gradient = 0.015259 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9238: Gradient = -0.008072 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9248: Gradient = -0.001068 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9258: Gradient = 0.005295 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9268: Gradient = 0.004562 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9278: Gradient = -0.003784 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9288: Gradient = -0.016525 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9329: Gradient = -0.005539 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9351: Gradient = -0.025482 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9367: Gradient = -0.022949 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9377: Gradient = -0.016052 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9387: Gradient = 0.017059 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9397: Gradient = -0.008698 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9407: Gradient = 0.003876 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9417: Gradient = 0.006821 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9427: Gradient = -0.003174 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9437: Gradient = -0.001633 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9447: Gradient = 0.001678 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9457: Gradient = -0.024567 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9495: Gradient = -0.035507 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9517: Gradient = 0.001099 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9532: Gradient = -0.042557 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9542: Gradient = -0.018692 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9552: Gradient = 0.023987 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9562: Gradient = -0.015884 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9572: Gradient = -0.000946 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9582: Gradient = 0.008331 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9592: Gradient = 0.003967 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9602: Gradient = 0.004562 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9612: Gradient = -0.000687 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9622: Gradient = -0.003815 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9632: Gradient = -0.037003 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9693: Gradient = 0.032486 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9707: Gradient = 0.046265 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9717: Gradient = 0.046127 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9727: Gradient = -0.040375 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9737: Gradient = 0.031143 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9747: Gradient = -0.008545 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9757: Gradient = 0.005539 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9767: Gradient = 0.001022 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9777: Gradient = 0.000305 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9787: Gradient = 0.002625 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9797: Gradient = 0.001495 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9807: Gradient = -0.003922 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9817: Gradient = -0.033447 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9868: Gradient = -0.021484 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9883: Gradient = 0.002945 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9899: Gradient = -0.017578 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9909: Gradient = -0.003189 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9919: Gradient = 0.009933 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9929: Gradient = 0.008484 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9939: Gradient = -0.001877 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9949: Gradient = 0.001511 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9959: Gradient = 0.008026 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9969: Gradient = 0.003662 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 9979: Gradient = 0.010178 < 0.05\n",
      "  → Increased batch size: 95 → 190\n",
      "  → Decreased learning rate: 0.001000 → 0.000500\n",
      "Epoch 10000:\n",
      "Train → MSE = 0.1182, RMSE = 0.3438, MAE = 0.2391, R² = 1.0000\n",
      "Val   → MSE = 952.7816, RMSE = 30.8672, MAE = 25.4618, R² = 0.9638\n",
      "\n",
      "Final Test Evaluation:\n",
      "MSE = 36.8057, RMSE = 6.0668, MAE = 5.1652, R² Score = 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/25 22:03:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/07/25 22:03:53 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94530a9fe31d40488cbef7644ae3247f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'FNN_Batch_Training' already exists. Creating a new version of this model...\n",
      "2025/07/25 22:03:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: FNN_Batch_Training, version 5\n",
      "Created version '5' of model 'FNN_Batch_Training'.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Adaptive Batching Summary:\n",
      "  Initial batch size: 16\n",
      "  Final batch size: 190\n",
      "  Initial learning rate: 0.005000\n",
      "  Final learning rate: 0.000500\n",
      "  Number of batch size increases: 4\n",
      "\n",
      "⏳ Training completed in 125.01 seconds.\n",
      "🏃 View run amazing-grouse-987 at: http://localhost:5000/#/experiments/408745732833894938/runs/e1bab8451a7f4833b235f83633a3da8d\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/408745732833894938\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "os.chdir('./experiments/neural_networks') if not load_dotenv() else None\n",
    "load_dotenv()\n",
    "print(f\"load_dotenv() returned: {load_dotenv()}\")\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(os.getenv(\"MLFLOW_EXPERIMENT_NAME\"))\n",
    "\n",
    "# --- MODIFIED: Changed lr and batch_size to initial values ---\n",
    "lr = 0.005  # This will be the initial learning rate, will adapt during training\n",
    "lr_min = 0.0005\n",
    "batch_size = 16  # CHANGED: Initial batch size instead of fixed batch size\n",
    "grad_threshold=0.05\n",
    "batch_multiplier=2\n",
    "lr_multiplier=0.5\n",
    "width_1st_layer = 32        \n",
    "width_2nd_layer = 16\n",
    "width_3rd_layer = 8\n",
    "width_4th_layer = 4     \n",
    "width_output_layer = 1      \n",
    "\n",
    "# --- MODIFIED: Updated params to reflect adaptive nature ---\n",
    "params = {\n",
    "    \"dataset\": data_file_name,\n",
    "    \"width_1st_layer\": width_1st_layer,\n",
    "    \"width_2nd_layer\": width_2nd_layer,\n",
    "    \"width_3rd_layer\": width_3rd_layer,\n",
    "    \"width_4th_layer\": width_4th_layer,\n",
    "    \"width_output_layer\": width_output_layer,\n",
    "    \"activation\": \"relu\",\n",
    "    \"criterion\": \"Mean Squared Error (MSE) loss\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"adaptive_batching_learning_rate\": True,  # ADDED: Flag to indicate adaptive batching\n",
    "    \"initial_lr\": lr,  # CHANGED: Renamed to initial_lr\n",
    "    \"initial_batch_size\": batch_size,  # CHANGED: Renamed to initial_batch_size\n",
    "    \"max_batch_size\": len(X_train),  # ADDED: Max batch size\n",
    "    \"grad_threshold\": grad_threshold,\n",
    "    \"batch_multiplier\": batch_multiplier,\n",
    "    \"lr_multiplier\": lr_multiplier,\n",
    "    \"lr_min\": lr_min\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Make it reproducible (unchanged)\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Define the Neural Network Model (unchanged)\n",
    "    class FeedforwardNN(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(FeedforwardNN, self).__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, width_1st_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width_1st_layer, width_2nd_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width_2nd_layer, width_3rd_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width_3rd_layer, width_4th_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width_4th_layer, width_output_layer),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "\n",
    "    model = FeedforwardNN(input_size=X_train.shape[1])\n",
    "    def init_weights(m):\n",
    "        '''Make the weight initialization reproducible'''\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0.01)\n",
    "    model.apply(init_weights)      \n",
    "\n",
    "    # Make GPU work (unchanged)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move data to device (unchanged)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    # --- ADDED: Initialize adaptive batch trainer ---\n",
    "    adaptive_trainer = GradientAdaptiveBatchingLearning(\n",
    "        initial_batch_size=batch_size,\n",
    "        initial_lr=lr,\n",
    "        grad_threshold=grad_threshold,  # Tune this based on your loss scale\n",
    "        batch_multiplier=batch_multiplier,  # Conservative increase\n",
    "        lr_multiplier = lr_multiplier,\n",
    "        lr_min = lr_min,\n",
    "        max_batch_size=len(X_train)  # CHANGED: Use actual training set size\n",
    "    )\n",
    "    \n",
    "    # Create initial DataLoaders (unchanged structure, but will be recreated during training)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Move model to device (unchanged)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training with MSE & Logging RMSE, MAE (unchanged)\n",
    "    def compute_rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "    def compute_mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    def compute_mape(predictions, targets):\n",
    "        return torch.mean(torch.abs((predictions - targets) / targets) * 100)\n",
    "    def compute_r2(predictions, targets):\n",
    "        ss_res = torch.sum((targets - predictions) ** 2)\n",
    "        ss_tot = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "        return 1 - ss_res / ss_tot\n",
    "\n",
    "    # --- MODIFIED: Training loop with adaptive batching ---\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), adaptive_trainer.current_lr)  # CHANGED: Use adaptive LR\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10_000\n",
    "    trigger_times = 0\n",
    "    min_delta = 0# -0.01\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # ADDED: Track batch size and learning rate history\n",
    "    batch_size_history = []\n",
    "    lr_history = []\n",
    "\n",
    "    progress = tqdm(range(10_001), desc=\"Training\")\n",
    "    for epoch in progress:\n",
    "        # Training phase with batches (unchanged structure)\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / num_train_batches\n",
    "\n",
    "        # Validation phase with batches (unchanged)\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                val_output = model(batch_X)\n",
    "                val_loss = criterion(val_output, batch_y)\n",
    "                epoch_val_loss += val_loss.item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / num_val_batches\n",
    "        \n",
    "        # --- ADDED: Adaptive batch size and learning rate update ---\n",
    "        new_batch_size, new_lr = adaptive_trainer.update_params(avg_val_loss, epoch)\n",
    "        \n",
    "        # ADDED: Update optimizer learning rate if changed\n",
    "        if new_lr != optimizer.param_groups[0]['lr']:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "        \n",
    "        # ADDED: Create new dataloader if batch size changed\n",
    "        if new_batch_size != train_loader.batch_size:\n",
    "            train_loader = DataLoader(train_dataset, batch_size=new_batch_size, shuffle=True)\n",
    "            # Also update validation loader for consistency\n",
    "            val_loader = DataLoader(val_dataset, batch_size=new_batch_size, shuffle=False)\n",
    "        \n",
    "        # ADDED: Track history\n",
    "        batch_size_history.append(new_batch_size)\n",
    "        lr_history.append(new_lr)\n",
    "        \n",
    "        # For progress display (unchanged)\n",
    "        with torch.no_grad():\n",
    "            sample_output = model(X_train[:new_batch_size])  # CHANGED: Use current batch size\n",
    "            sample_rmse = compute_rmse(sample_output, y_train[:new_batch_size])\n",
    "            # MODIFIED: Show current batch size in progress\n",
    "            progress.set_postfix({\n",
    "                \"Loss\": avg_train_loss, \n",
    "                \"RMSE\": sample_rmse.item(),\n",
    "                \"BatchSize\": new_batch_size,  # ADDED: Show current batch size\n",
    "                \"LR\": f\"{new_lr:.6f}\"  # ADDED: Show current learning rate\n",
    "            })\n",
    "        \n",
    "        # Save losses each epoch (unchanged)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Early stopping logic (unchanged)\n",
    "        if avg_val_loss + min_delta < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f\"\\n⏹️ Early stopping at epoch {epoch} — no validation improvement after {patience} epochs.\")\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    train_output = model(X_train)\n",
    "                    val_output = model(X_val)\n",
    "                    train_loss_final = criterion(train_output, y_train)\n",
    "                    val_loss_final = criterion(val_output, y_val)\n",
    "                    \n",
    "                    rmse = compute_rmse(train_output, y_train)\n",
    "                    mae = compute_mae(train_output, y_train)\n",
    "                    r2 = compute_r2(train_output, y_train)\n",
    "                    val_rmse = compute_rmse(val_output, y_val)\n",
    "                    val_mae = compute_mae(val_output, y_val)\n",
    "                    val_r2 = compute_r2(val_output, y_val)\n",
    "                    \n",
    "                print(f\"Train → MSE = {train_loss_final.item():.4f}, RMSE = {rmse.item():.4f}, MAE = {mae.item():.4f}, R² = {r2.item():.4f}\")\n",
    "                print(f\"Val   → MSE = {val_loss_final.item():.4f}, RMSE = {val_rmse.item():.4f}, MAE = {val_mae.item():.4f}, R² = {val_r2.item():.4f}\")\n",
    "                break\n",
    "\n",
    "        if epoch % 10_000 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_output = model(X_train)\n",
    "                val_output = model(X_val)\n",
    "                train_loss_full = criterion(train_output, y_train)\n",
    "                val_loss_full = criterion(val_output, y_val)\n",
    "                \n",
    "                rmse = compute_rmse(train_output, y_train)\n",
    "                mae = compute_mae(train_output, y_train)\n",
    "                r2 = compute_r2(train_output, y_train)\n",
    "                val_rmse = compute_rmse(val_output, y_val)\n",
    "                val_mae = compute_mae(val_output, y_val)\n",
    "                val_r2 = compute_r2(val_output, y_val)\n",
    "                \n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"Train → MSE = {train_loss_full.item():.4f}, RMSE = {rmse.item():.4f}, MAE = {mae.item():.4f}, R² = {r2.item():.4f}\")\n",
    "            print(f\"Val   → MSE = {val_loss_full.item():.4f}, RMSE = {val_rmse.item():.4f}, MAE = {val_mae.item():.4f}, R² = {val_r2.item():.4f}\")\n",
    "\n",
    "    # # Compute average loss per epoch (unchanged)\n",
    "    # avg_losses = [(train + val) / 2 for train, val in zip(train_losses, val_losses)]\n",
    "    # search_limit = 10_000\n",
    "    # limited_avg_losses = avg_losses[:search_limit]\n",
    "    # min_avg = min(limited_avg_losses)\n",
    "    # epoch_min_avg = limited_avg_losses.index(min_avg)\n",
    "    # print(f\"🔍 Minimum average loss before epoch {search_limit}: {min_avg:.4f} at epoch {epoch_min_avg}\")\n",
    "\n",
    "    # Metrics after all the epochs - training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get final predictions on full datasets\n",
    "        train_output = model(X_train)\n",
    "        val_output = model(X_val)\n",
    "        test_output = model(X_test)\n",
    "        \n",
    "        # Training metrics\n",
    "        training_mse = criterion(train_output, y_train)\n",
    "        training_rmse = compute_rmse(train_output, y_train)\n",
    "        training_mae = compute_mae(train_output, y_train)\n",
    "        training_mape = compute_mape(train_output, y_train)\n",
    "        training_r2_score = compute_r2(train_output, y_train)\n",
    "    \n",
    "        # Validation metrics\n",
    "        val_mse = criterion(val_output, y_val)\n",
    "        val_rmse = compute_rmse(val_output, y_val)\n",
    "        val_mae = compute_mae(val_output, y_val)\n",
    "        val_mape = compute_mape(val_output, y_val)\n",
    "        val_r2_score = compute_r2(val_output, y_val)\n",
    "\n",
    "        # Test metrics\n",
    "        test_loss = criterion(test_output, y_test)\n",
    "        test_rmse = compute_rmse(test_output, y_test)\n",
    "        test_mae = compute_mae(test_output, y_test)\n",
    "        test_mape = compute_mape(test_output, y_test)\n",
    "        test_r2 = compute_r2(test_output, y_test)\n",
    "        \n",
    "        print(f\"\\nFinal Test Evaluation:\")\n",
    "        print(f\"MSE = {test_loss.item():.4f}, RMSE = {test_rmse.item():.4f}, MAE = {test_mae.item():.4f}, R² Score = {test_r2.item():.4f}\")\n",
    "\n",
    "    # Log metrics to MLflow\n",
    "    # training\n",
    "    mlflow.log_metric(\"training_mean_squared_error\", round(training_mse.item(), 2))\n",
    "    mlflow.log_metric(\"training_root_mean_squared_error\", round(training_rmse.item(),2))\n",
    "    mlflow.log_metric(\"training_mean_absolute_error\", round(training_mae.item(),2))\n",
    "    mlflow.log_metric(\"training_mean_absolute_percentage_error\", round(training_mape.item(),2))\n",
    "    mlflow.log_metric(\"training_r2_score\", round(training_r2_score.item(),2))\n",
    "    # val\n",
    "    mlflow.log_metric(\"val_mean_squared_error\", round(val_mse.item(), 2))\n",
    "    mlflow.log_metric(\"val_rmse\", round(val_rmse.item(),2))\n",
    "    mlflow.log_metric(\"val_mae\", round(val_mae.item(),2))\n",
    "    mlflow.log_metric(\"val_mape\", round(val_mape.item(),2))\n",
    "    mlflow.log_metric(\"val_r2_score\", round(val_r2_score.item(),2))\n",
    "    #test\n",
    "    mlflow.log_metric(\"test_mse\", round(test_loss.item(),2))\n",
    "    mlflow.log_metric(\"test_rmse\", round(test_rmse.item(),2))\n",
    "    mlflow.log_metric(\"test_mae\", round(test_mae.item(),2))\n",
    "    mlflow.log_metric(\"test_mape\", round(test_mape.item(),2))\n",
    "    mlflow.log_metric(\"test_r2_score\", round(test_r2.item(),2))\n",
    "\n",
    "    # Log the model\n",
    "    X_train_numpy = X_train.detach().cpu().numpy()\n",
    "    train_output_numpy = train_output.detach().cpu().numpy()\n",
    "    signature = infer_signature(X_train_numpy, train_output_numpy)\n",
    "    \n",
    "    mlflow.pytorch.log_model(\n",
    "        model,\n",
    "        name=\"feed_forward_neural_network\",\n",
    "        signature=signature,\n",
    "        input_example=X_train_numpy[:5],\n",
    "        registered_model_name=\"FNN_Batch_Training\"\n",
    "    )\n",
    "\n",
    "    # Model summary/architecture\n",
    "    with open(\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(model))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "    # --- MODIFIED: Enhanced plotting with adaptive batching info ---\n",
    "    start = 100 \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves (existing plot)\n",
    "    ax1.plot(range(start, len(train_losses)), train_losses[start:], label='Training Loss', color='blue')\n",
    "    ax1.plot(range(start, len(val_losses)), val_losses[start:], label='Validation Loss', color='orange')\n",
    "    #ax1.plot(range(start, len(avg_losses)), avg_losses[start:], label='Avg Loss', color='purple')\n",
    "    #ax1.axvline(x=epoch_min_avg, color='red', linestyle='--', label='Min Avg Loss')\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Loss Curves\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    plt.savefig(\"loss_curves.png\")\n",
    "    mlflow.log_artifact(\"loss_curves.png\")\n",
    "    \n",
    "    # ADDED: Batch size evolution\n",
    "    ax2.plot(batch_size_history, color='green')\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Batch Size\")\n",
    "    ax2.set_title(\"Batch Size Evolution\")\n",
    "    ax2.grid(True)\n",
    "    plt.savefig(\"batch_size_evolution.png\")\n",
    "    mlflow.log_artifact(\"batch_size_evolution.png\")\n",
    "    \n",
    "    # ADDED: Learning rate evolution\n",
    "    ax3.plot(lr_history, color='red')\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Learning Rate\")\n",
    "    ax3.set_title(\"Learning Rate Evolution\")\n",
    "    ax3.grid(True)\n",
    "    plt.savefig(\"learning_rate_evolution.png\")\n",
    "    mlflow.log_artifact(\"learning_rate_evolution.png\")\n",
    "    \n",
    "    # ADDED: Validation loss gradient (to visualize why batch size changed)\n",
    "    if len(val_losses) > 50:\n",
    "        val_gradients = np.diff(val_losses)\n",
    "        ax4.plot(val_gradients[-min(10000, len(val_gradients)):], color='orange', alpha=0.7)\n",
    "        ax4.axhline(y=adaptive_trainer.grad_threshold, color='red', linestyle='--', \n",
    "                   label=f'Threshold: {adaptive_trainer.grad_threshold}')\n",
    "        ax4.axhline(y=-adaptive_trainer.grad_threshold, color='red', linestyle='--')\n",
    "        ax4.set_xlabel(\"Epoch\")\n",
    "        ax4.set_ylabel(\"Validation Loss Gradient\")\n",
    "        ax4.set_title(\"Recent Validation Loss Gradient\")\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "        plt.savefig(\"validation_loss_gradient.png\")\n",
    "        mlflow.log_artifact(\"validation_loss_gradient.png\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- ADDED: Log adaptive batching metrics ---\n",
    "    mlflow.log_metric(\"final_batch_size\", batch_size_history[-1])\n",
    "    mlflow.log_metric(\"final_learning_rate\", lr_history[-1])\n",
    "    mlflow.log_metric(\"batch_size_increases\", len(set(batch_size_history)) - 1)\n",
    "    \n",
    "    # ADDED: Print summary of adaptive batching\n",
    "    print(f\"\\n📊 Adaptive Batching Summary:\")\n",
    "    print(f\"  Initial batch size: {batch_size}\")\n",
    "    print(f\"  Final batch size: {batch_size_history[-1]}\")\n",
    "    print(f\"  Initial learning rate: {lr:.6f}\")\n",
    "    print(f\"  Final learning rate: {lr_history[-1]:.6f}\")\n",
    "    print(f\"  Number of batch size increases: {len(set(batch_size_history)) - 1}\")\n",
    "\n",
    "    # Clean memory in case we want to run this cell again without running the whole notebook\n",
    "    # remove references to GPU objects \n",
    "    del model\n",
    "    # Invoke garbage collector\n",
    "    gc.collect()\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"\\n⏳ Training completed in {elapsed_time:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtx1060_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
