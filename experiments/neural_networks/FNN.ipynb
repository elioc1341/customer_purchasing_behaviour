{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986ab7cc",
   "metadata": {},
   "source": [
    "**Feedforward Neural Network (FNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fc4c9",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "#from tqdm import tqdm, trange\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b27da",
   "metadata": {},
   "source": [
    "Load csv into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f040e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\The Winner\\DSI\\customer_purchasing_behaviour\n",
      "Successfully loaded 'Customer_Purchasing_Behaviors.csv' into the DataFrame named df.\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure consistent working directory for data loading ---\n",
    "# This block dynamically sets the current working directory to the Git repository root.\n",
    "# This makes data paths reliable for all collaborators, regardless of where they open the notebook.\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_root = current_dir\n",
    "while not os.path.exists(os.path.join(repo_root, '.git')):\n",
    "    # Move up one directory\n",
    "    parent_dir = os.path.dirname(repo_root)\n",
    "    if parent_dir == repo_root: # Reached filesystem root, .git not found\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the .git directory. \"\n",
    "            \"Please ensure you are running this code from within a Git repository.\"\n",
    "        )\n",
    "    repo_root = parent_dir\n",
    "\n",
    "# Change the current working directory if it's not already the repo root\n",
    "if os.getcwd() != repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\") # Informative print for users\n",
    "\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Path to the data file, relative to the repository root.\n",
    "data_file_name = 'Customer_Purchasing_Behaviors.csv'\n",
    "data_file_path = os.path.join('data', 'raw', data_file_name)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    print(f\"Successfully loaded '{data_file_name}' into the DataFrame named df.\")\n",
    "    #print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{data_file_name}' was not found at '{data_file_path}'.\")\n",
    "    print(\"Please ensure it exists in the 'data/raw/' folder relative to the repository root.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaad9b8",
   "metadata": {},
   "source": [
    "Prepare data: scale numerical and encode categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9866895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Prepare Inputs\n",
    "\n",
    "# Extract relevant columns\n",
    "numerical_cols = ['age', 'annual_income', 'loyalty_score', 'purchase_frequency']\n",
    "categorical_col = 'region'\n",
    "target_col = 'purchase_amount'\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Encode categorical feature\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_cat = encoder.fit_transform(df[[categorical_col]])\n",
    "\n",
    "# Create DataFrames with distinct column names\n",
    "num_df = pd.DataFrame(X_num, columns=numerical_cols)\n",
    "cat_columns = encoder.get_feature_names_out([categorical_col])\n",
    "cat_df = pd.DataFrame(X_cat, columns=cat_columns)\n",
    "\n",
    "# Combine safely\n",
    "combined_df = pd.concat([num_df, cat_df], axis=1)\n",
    "\n",
    "# Convert to torch tensor\n",
    "X_combined = torch.tensor(combined_df.values, dtype=torch.float32)\n",
    "\n",
    "# Target variable\n",
    "y = torch.tensor(df[target_col].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split into Train + Validation, and Test (80% train+val, 20% test)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b52f03",
   "metadata": {},
   "source": [
    "Run Model and log it in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load environment variables\n",
    "os.chdir('./experiments/neural_networks') if not load_dotenv() else None\n",
    "load_dotenv()\n",
    "print(f\"load_dotenv() returned: {load_dotenv()}\")  # Should be True if file found\n",
    "# Set tracking URI (where MLflow server is running)\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))  # Note: Not ARTIFACT_ROOT\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(os.getenv(\"MLFLOW_EXPERIMENT_NAME\"))\n",
    "\n",
    "#mlflow.autolog() # Prefer to run manually, less items logged\n",
    "\n",
    "lr = 0.005\n",
    "batch_size = 32  # Added batch size parameter\n",
    "width_1st_layer = 8        # 64\n",
    "width_2nd_layer = 8        # 32\n",
    "width_output_layer = 1      # 1\n",
    "\n",
    "params = {    \n",
    "    \"width_1st_layer\": width_1st_layer,\n",
    "    \"width_2nd_layer\": width_2nd_layer,\n",
    "    \"width_output_layer\": width_output_layer,\n",
    "    \"activation\": \"relu\",\n",
    "    \"criterion\": \"Mean Squared Error (MSE) loss\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": lr,\n",
    "    \"batch_size\": batch_size  # Added to parameters\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Make it reproducible\n",
    "    SEED = 42\n",
    "    random.seed(SEED)                 # Python random\n",
    "    np.random.seed(SEED)              # NumPy\n",
    "    torch.manual_seed(SEED)           # CPU\n",
    "    torch.cuda.manual_seed(SEED)      # GPU\n",
    "    torch.cuda.manual_seed_all(SEED)  # Multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Define the Neural Network Model\n",
    "    class FeedforwardNN(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(FeedforwardNN, self).__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, width_1st_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width_1st_layer, width_2nd_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width_2nd_layer, width_output_layer)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "\n",
    "    model = FeedforwardNN(input_size=X_train.shape[1])\n",
    "    def init_weights(m):\n",
    "        '''Make the weight initialization reproducible'''\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)  # still uses seed\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0.01)\n",
    "    model.apply(init_weights)      \n",
    "\n",
    "    # Make GPU work\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = \"cpu\"\n",
    "    \n",
    "    # Move data to device\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    # Create DataLoaders for batch training\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training with MSE & Logging RMSE, MAE \n",
    "    # Metrics\n",
    "    def compute_rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "    def compute_mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    def compute_r2(predictions, targets):\n",
    "        ss_res = torch.sum((targets - predictions) ** 2)\n",
    "        ss_tot = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "        return 1 - ss_res / ss_tot\n",
    "\n",
    "    # Training + Early Stopping\n",
    "    criterion = nn.MSELoss()   # Mean Squared Error (MSE) loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10_000\n",
    "    trigger_times = 0\n",
    "    min_delta = -0.05\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    progress = tqdm(range(10_001), desc=\"Training\")\n",
    "    for epoch in progress:\n",
    "        # Training phase with batches\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "        \n",
    "        # Average training loss for the epoch\n",
    "        avg_train_loss = epoch_train_loss / num_train_batches\n",
    "\n",
    "        # Validation phase with batches\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                val_output = model(batch_X)\n",
    "                val_loss = criterion(val_output, batch_y)\n",
    "                epoch_val_loss += val_loss.item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        # Average validation loss for the epoch\n",
    "        avg_val_loss = epoch_val_loss / num_val_batches\n",
    "        \n",
    "        # For progress display, get RMSE on a sample batch\n",
    "        with torch.no_grad():\n",
    "            sample_output = model(X_train[:batch_size])  # Use first batch_size samples\n",
    "            sample_rmse = compute_rmse(sample_output, y_train[:batch_size])\n",
    "            progress.set_postfix({\"Loss\": avg_train_loss, \"RMSE\": sample_rmse.item()})\n",
    "        \n",
    "        # Save losses each epoch\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_val_loss + min_delta < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch} ‚Äî no validation improvement after {patience} epochs.\")\n",
    "                \n",
    "                # Calculate final metrics on full datasets\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    train_output = model(X_train)\n",
    "                    val_output = model(X_val)\n",
    "                    train_loss_final = criterion(train_output, y_train)\n",
    "                    val_loss_final = criterion(val_output, y_val)\n",
    "                    \n",
    "                    rmse = compute_rmse(train_output, y_train)\n",
    "                    mae = compute_mae(train_output, y_train)\n",
    "                    r2 = compute_r2(train_output, y_train)\n",
    "                    val_rmse = compute_rmse(val_output, y_val)\n",
    "                    val_mae = compute_mae(val_output, y_val)\n",
    "                    val_r2 = compute_r2(val_output, y_val)\n",
    "                    \n",
    "                print(f\"Train ‚Üí MSE = {train_loss_final.item():.4f}, RMSE = {rmse.item():.4f}, MAE = {mae.item():.4f}, R¬≤ = {r2.item():.4f}\")\n",
    "                print(f\"Val   ‚Üí MSE = {val_loss_final.item():.4f}, RMSE = {val_rmse.item():.4f}, MAE = {val_mae.item():.4f}, R¬≤ = {val_r2.item():.4f}\")\n",
    "                break\n",
    "\n",
    "        if epoch % 10_000 == 0:\n",
    "            # Calculate metrics on full datasets for reporting\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_output = model(X_train)\n",
    "                val_output = model(X_val)\n",
    "                train_loss_full = criterion(train_output, y_train)\n",
    "                val_loss_full = criterion(val_output, y_val)\n",
    "                \n",
    "                rmse = compute_rmse(train_output, y_train)\n",
    "                mae = compute_mae(train_output, y_train)\n",
    "                r2 = compute_r2(train_output, y_train)\n",
    "                val_rmse = compute_rmse(val_output, y_val)\n",
    "                val_mae = compute_mae(val_output, y_val)\n",
    "                val_r2 = compute_r2(val_output, y_val)\n",
    "                \n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"Train ‚Üí MSE = {train_loss_full.item():.4f}, RMSE = {rmse.item():.4f}, MAE = {mae.item():.4f}, R¬≤ = {r2.item():.4f}\")\n",
    "            print(f\"Val   ‚Üí MSE = {val_loss_full.item():.4f}, RMSE = {val_rmse.item():.4f}, MAE = {val_mae.item():.4f}, R¬≤ = {val_r2.item():.4f}\")\n",
    "\n",
    "    # Compute average loss per epoch\n",
    "    avg_losses = [(train + val) / 2 for train, val in zip(train_losses, val_losses)]\n",
    "    # Condition to only search within epochs < 7500\n",
    "    search_limit = 10_000\n",
    "    limited_avg_losses = avg_losses[:search_limit]\n",
    "    # Find the minimum within this range\n",
    "    min_avg = min(limited_avg_losses)\n",
    "    epoch_min_avg = limited_avg_losses.index(min_avg)\n",
    "    print(f\"üîç Minimum average loss before epoch {search_limit}: {min_avg:.4f} at epoch {epoch_min_avg}\")\n",
    "\n",
    "    # Final Evaluation on Test Set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(X_test)\n",
    "        test_loss = criterion(test_predictions, y_test)\n",
    "        test_rmse = compute_rmse(test_predictions, y_test)\n",
    "        test_mae = compute_mae(test_predictions, y_test)\n",
    "        test_r2 = compute_r2(test_predictions, y_test)\n",
    "        print(f\"\\nFinal Test Evaluation:\")\n",
    "        print(f\"MSE = {test_loss.item():.4f}, RMSE = {test_rmse.item():.4f}, MAE = {test_mae.item():.4f}, R¬≤ Score = {test_r2.item():.4f}\")\n",
    "\n",
    "    # Plot after training\n",
    "    start = 50 \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    #plt.yscale('log')\n",
    "    plt.plot(range(start, len(train_losses)), train_losses[start:], label='Training Loss', color='blue')\n",
    "    plt.plot(range(start, len(val_losses)), val_losses[start:], label='Validation Loss', color='orange')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    # Draw line minimum average training and validation losses\n",
    "    plt.plot(range(start, len(avg_losses)), avg_losses[start:], label='Avg Loss', color='purple')\n",
    "    plt.axvline(x=epoch_min_avg, color='red', linestyle='--', label='Min Avg Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Clean memory in case we want to run this cell again without running the whole notebook\n",
    "    # remove references to GPU objects \n",
    "    del model, avg_losses, limited_avg_losses, min_avg, train_losses, val_losses\n",
    "    # Invoke garbage collector\n",
    "    #gc.collect()\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #########################################################################################################\n",
    "    #########################################################################################################\n",
    "    # Now that I know the number of epochs optimum where to stop, I am going to calculate it again and stop there\n",
    "\n",
    "    # Make it reproducible\n",
    "    SEED = 42\n",
    "    random.seed(SEED)                 # Python random\n",
    "    np.random.seed(SEED)              # NumPy\n",
    "    torch.manual_seed(SEED)           # CPU\n",
    "    torch.cuda.manual_seed(SEED)      # GPU\n",
    "    torch.cuda.manual_seed_all(SEED)  # Multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Define the Neural Network Model\n",
    "    class FeedforwardNN(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(FeedforwardNN, self).__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, width_1st_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width_1st_layer, width_2nd_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width_2nd_layer, width_output_layer)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "\n",
    "    model = FeedforwardNN(input_size=X_train.shape[1])\n",
    "    def init_weights(m):\n",
    "        '''Make the weight initialization reproducible'''\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)  # still uses seed\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0.01)\n",
    "    model.apply(init_weights)      \n",
    "\n",
    "    # Make GPU work\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = \"cpu\"\n",
    "    \n",
    "    # Move data to device (already moved above, but keeping for clarity)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    # Recreate DataLoaders for second training\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training with MSE & Logging RMSE, MAE \n",
    "    # Metrics\n",
    "    def compute_rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "    def compute_mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    def compute_mape(predictions, targets):\n",
    "        return torch.mean(torch.abs((predictions - targets) / targets) * 100)\n",
    "    def compute_r2(predictions, targets):\n",
    "        ss_res = torch.sum((targets - predictions) ** 2)\n",
    "        ss_tot = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "        return 1 - ss_res / ss_tot\n",
    "\n",
    "    # Training + Early Stopping\n",
    "    criterion = nn.MSELoss()   # Mean Squared Error (MSE) loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    progress = tqdm(range(epoch_min_avg+1), desc=\"Training\")\n",
    "    for epoch in progress:\n",
    "        # Training phase with batches\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "        \n",
    "        # Average training loss for the epoch\n",
    "        avg_train_loss = epoch_train_loss / num_train_batches\n",
    "\n",
    "        # Validation phase with batches\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                val_output = model(batch_X)\n",
    "                val_loss = criterion(val_output, batch_y)\n",
    "                epoch_val_loss += val_loss.item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        # Average validation loss for the epoch\n",
    "        avg_val_loss = epoch_val_loss / num_val_batches\n",
    "        \n",
    "        # For progress display, get RMSE on a sample\n",
    "        with torch.no_grad():\n",
    "            sample_output = model(X_train[:batch_size])\n",
    "            sample_rmse = compute_rmse(sample_output, y_train[:batch_size])\n",
    "            progress.set_postfix({\"Loss\": avg_train_loss, \"RMSE\": sample_rmse.item()})\n",
    "        \n",
    "        # Save losses each epoch\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if epoch % 10_000 == 0:\n",
    "            # Calculate metrics on full datasets for reporting\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_output = model(X_train)\n",
    "                val_output = model(X_val)\n",
    "                train_loss_full = criterion(train_output, y_train)\n",
    "                val_loss_full = criterion(val_output, y_val)\n",
    "                \n",
    "                rmse = compute_rmse(train_output, y_train)\n",
    "                mae = compute_mae(train_output, y_train)\n",
    "                r2 = compute_r2(train_output, y_train)\n",
    "                val_rmse = compute_rmse(val_output, y_val)\n",
    "                val_mae = compute_mae(val_output, y_val)\n",
    "                val_r2 = compute_r2(val_output, y_val)\n",
    "                \n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"Train ‚Üí MSE = {train_loss_full.item():.4f}, RMSE = {rmse.item():.4f}, MAE = {mae.item():.4f}, R¬≤ = {r2.item():.4f}\")\n",
    "            print(f\"Val   ‚Üí MSE = {val_loss_full.item():.4f}, RMSE = {val_rmse.item():.4f}, MAE = {val_mae.item():.4f}, R¬≤ = {val_r2.item():.4f}\")\n",
    "\n",
    "    # Compute average loss per epoch\n",
    "    avg_losses = [(train + val) / 2 for train, val in zip(train_losses, val_losses)]\n",
    "\n",
    "    # Metrics after all the epochs - training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get final predictions on full datasets\n",
    "        train_output = model(X_train)\n",
    "        val_output = model(X_val)\n",
    "        test_output = model(X_test)\n",
    "        \n",
    "        # Training metrics\n",
    "        training_mse = criterion(train_output, y_train)\n",
    "        training_rmse = compute_rmse(train_output, y_train)\n",
    "        training_mae = compute_mae(train_output, y_train)\n",
    "        training_mape = compute_mape(train_output, y_train)\n",
    "        training_r2_score = compute_r2(train_output, y_train)\n",
    "    \n",
    "        # Validation metrics\n",
    "        val_mse = criterion(val_output, y_val)\n",
    "        val_rmse = compute_rmse(val_output, y_val)\n",
    "        val_mae = compute_mae(val_output, y_val)\n",
    "        val_mape = compute_mape(val_output, y_val)\n",
    "        val_r2_score = compute_r2(val_output, y_val)\n",
    "\n",
    "        # Test metrics\n",
    "        test_loss = criterion(test_output, y_test)\n",
    "        test_rmse = compute_rmse(test_output, y_test)\n",
    "        test_mae = compute_mae(test_output, y_test)\n",
    "        test_mape = compute_mape(test_output, y_test)\n",
    "        test_r2 = compute_r2(test_output, y_test)\n",
    "        \n",
    "        print(f\"\\nFinal Test Evaluation:\")\n",
    "        print(f\"MSE = {test_loss.item():.4f}, RMSE = {test_rmse.item():.4f}, MAE = {test_mae.item():.4f}, R¬≤ Score = {test_r2.item():.4f}\")\n",
    "\n",
    "    # Log metrics to MLflow\n",
    "    # training\n",
    "    mlflow.log_metric(\"training_mean_squared_error\", round(training_mse.item(), 2))\n",
    "    mlflow.log_metric(\"training_root_mean_squared_error\", round(training_rmse.item(),2))\n",
    "    mlflow.log_metric(\"training_mean_absolute_error\", round(training_mae.item(),2))\n",
    "    mlflow.log_metric(\"training_mean_absolute_percentage_error\", round(training_mape.item(),2))\n",
    "    mlflow.log_metric(\"training_r2_score\", round(training_r2_score.item(),2))\n",
    "    # val\n",
    "    mlflow.log_metric(\"val_mean_squared_error\", round(val_mse.item(), 2))\n",
    "    mlflow.log_metric(\"val_rmse\", round(val_rmse.item(),2))\n",
    "    mlflow.log_metric(\"val_mae\", round(val_mae.item(),2))\n",
    "    mlflow.log_metric(\"val_mape\", round(val_mape.item(),2))\n",
    "    mlflow.log_metric(\"val_r2_score\", round(val_r2_score.item(),2))\n",
    "    #test\n",
    "    mlflow.log_metric(\"test_mse\", round(test_loss.item(),2))\n",
    "    mlflow.log_metric(\"test_rmse\", round(test_rmse.item(),2))\n",
    "    mlflow.log_metric(\"test_mae\", round(test_mae.item(),2))\n",
    "    mlflow.log_metric(\"test_mape\", round(test_mape.item(),2))\n",
    "    mlflow.log_metric(\"test_r2_score\", round(test_r2.item(),2))\n",
    "\n",
    "    # Log the model\n",
    "    X_train_numpy = X_train.detach().cpu().numpy()\n",
    "    train_output_numpy = train_output.detach().cpu().numpy()\n",
    "    signature = infer_signature(X_train_numpy, train_output_numpy)\n",
    "    \n",
    "    mlflow.pytorch.log_model(\n",
    "        model,\n",
    "        name=\"feed_forward_neural_network\",\n",
    "        signature=signature,\n",
    "        input_example=X_train_numpy[:5],\n",
    "        registered_model_name=\"FNN_Batch_Training\"\n",
    "    )\n",
    "\n",
    "    # Plot after training\n",
    "    start = 4_000 \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.yscale('log')\n",
    "    plt.plot(range(start, len(train_losses)), train_losses[start:], label='Training Loss', color='blue')\n",
    "    plt.plot(range(start, len(val_losses)), val_losses[start:], label='Validation Loss', color='orange')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    # Draw line minimum average training and validation losses\n",
    "    plt.plot(range(start, len(avg_losses)), avg_losses[start:], label='Avg Loss', color='purple')\n",
    "    plt.axvline(x=epoch_min_avg, color='red', linestyle='--', label='Min Avg Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    mlflow.log_artifact(\"loss_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Model summary/architecture\n",
    "    with open(\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(model))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "    # Clean memory in case we want to run this cell again without running the whole notebook\n",
    "    # remove references to GPU objects \n",
    "    del model\n",
    "    # Invoke garbage collector\n",
    "    gc.collect()\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"\\n‚è≥ Training completed in {elapsed_time:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtx1060_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
