{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1454a24",
   "metadata": {},
   "source": [
    "Claude Bot - Optuna Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a9f99",
   "metadata": {},
   "source": [
    "Optuna with Your Exact Current Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a787c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_your_style(trial):\n",
    "    # Suggest hyperparameters\n",
    "    neurons = trial.suggest_int('neurons', 4, 64)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    \n",
    "    # Use your existing 4-fold CV on X_train\n",
    "    cv_scores = []\n",
    "    kfold = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X_train):\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model = create_model(neurons=neurons, dropout=dropout)\n",
    "        # ... train model ...\n",
    "        fold_score = evaluate_model(model, X_fold_val, y_fold_val)\n",
    "        cv_scores.append(fold_score)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Then proceed with your existing workflow:\n",
    "study = optuna.create_study(direction='minimize')\n",
    "best_params = study.optimize(objective_your_style, n_trials=100)\n",
    "\n",
    "# Train final model on full X_train with best params\n",
    "final_model = train_final_model(best_params, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluate on X_test (once)\n",
    "test_loss = final_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ead4c9",
   "metadata": {},
   "source": [
    "Recommendation for Your Case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89658ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best of both worlds: Optuna + your existing CV workflow\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'neurons': trial.suggest_int('neurons', 4, 64),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-1, log=True),\n",
    "        'dropout': trial.suggest_float('dropout', 0.0, 0.5),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    }\n",
    "    \n",
    "    # Your existing k-fold evaluation\n",
    "    return k_fold_evaluate(params, X_train, y_train, k=4)\n",
    "\n",
    "# Let Optuna find best hyperparameters more efficiently\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Continue with your workflow using best params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae375d",
   "metadata": {},
   "source": [
    "Integrating Both Systems  \n",
    "Here's how to combine Optuna with your adaptive system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveTrainer:\n",
    "    def __init__(self, initial_lr, initial_batch_size, lr_multiplier=0.5, \n",
    "                 batch_multiplier=2, grad_threshold=1, patience=10):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.initial_batch_size = initial_batch_size\n",
    "        self.lr_multiplier = lr_multiplier\n",
    "        self.batch_multiplier = batch_multiplier\n",
    "        self.grad_threshold = grad_threshold\n",
    "        self.patience = patience\n",
    "        \n",
    "    def train_with_adaptation(self, model, X_train, y_train, X_val, y_val, epochs=100):\n",
    "        current_lr = self.initial_lr\n",
    "        current_batch_size = self.initial_batch_size\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=current_lr)\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train one epoch\n",
    "            epoch_loss = self.train_epoch(model, optimizer, X_train, y_train, \n",
    "                                        current_batch_size)\n",
    "            val_loss = self.evaluate(model, X_val, y_val)\n",
    "            loss_history.append(val_loss)\n",
    "            \n",
    "            # Check if we should adapt\n",
    "            if self.should_adapt(loss_history):\n",
    "                current_lr *= self.lr_multiplier\n",
    "                current_batch_size = min(current_batch_size * self.batch_multiplier, \n",
    "                                       len(X_train))  # Don't exceed dataset size\n",
    "                \n",
    "                # Update optimizer with new learning rate\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "                    \n",
    "                print(f\"Epoch {epoch}: Adapted lr={current_lr:.6f}, \"\n",
    "                      f\"batch_size={current_batch_size}\")\n",
    "        \n",
    "        return loss_history\n",
    "    \n",
    "    def should_adapt(self, loss_history):\n",
    "        if len(loss_history) < self.patience:\n",
    "            return False\n",
    "            \n",
    "        # Calculate slope of recent losses\n",
    "        recent_losses = loss_history[-self.patience:]\n",
    "        x = np.arange(len(recent_losses))\n",
    "        slope = np.polyfit(x, recent_losses, 1)[0]\n",
    "        \n",
    "        return abs(slope) < self.grad_threshold\n",
    "\n",
    "def objective_with_adaptive_training(trial):\n",
    "    # Optuna tunes INITIAL parameters and adaptation settings\n",
    "    initial_lr = trial.suggest_float('initial_lr', 1e-4, 1e-1, log=True)\n",
    "    initial_batch_size = trial.suggest_categorical('initial_batch_size', [8, 16, 32])\n",
    "    \n",
    "    # Optuna can also tune your adaptation strategy\n",
    "    lr_multiplier = trial.suggest_float('lr_multiplier', 0.3, 0.8)\n",
    "    batch_multiplier = trial.suggest_float('batch_multiplier', 1.5, 3.0)\n",
    "    grad_threshold = trial.suggest_float('grad_threshold', 0.0001, 0.01, log=True)\n",
    "    patience = trial.suggest_int('patience', 5, 20)\n",
    "    \n",
    "    # Cross-validation with adaptive training\n",
    "    kfold = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X_train):\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model = create_model()\n",
    "        trainer = AdaptiveTrainer(\n",
    "            initial_lr=initial_lr,\n",
    "            initial_batch_size=initial_batch_size,\n",
    "            lr_multiplier=lr_multiplier,\n",
    "            batch_multiplier=batch_multiplier,\n",
    "            grad_threshold=grad_threshold,\n",
    "            patience=patience\n",
    "        )\n",
    "        \n",
    "        # Train with adaptation\n",
    "        loss_history = trainer.train_with_adaptation(\n",
    "            model, X_fold_train, y_fold_train, X_fold_val, y_fold_val\n",
    "        )\n",
    "        \n",
    "        cv_scores.append(loss_history[-1])\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective_with_adaptive_training, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a63a60",
   "metadata": {},
   "source": [
    "Look at all code after:   \n",
    "In study.optimize(...,  n_trials=100), if you input n_trials=100_000, does optuna stop when already tried the most meaningful combinations or is optuna actually going to run each of the 100_000 trials?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
