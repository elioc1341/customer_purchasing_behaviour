{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67873f7a",
   "metadata": {},
   "source": [
    "**0. Load and Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e18bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# set visulization style\n",
    "sns.set_style(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b6b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\The Winner\\DSI\\customer_purchasing_behaviour\n",
      "Successfully loaded 'Customer_Purchasing_Behaviors.csv'.\n",
      "Original DataFrame shape: (238, 7)\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure consistent working directory for data loading ---\n",
    "# This block dynamically sets the current working directory to the Git repository root.\n",
    "# This makes data paths reliable for all collaborators, regardless of where they open the notebook.\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_root = current_dir\n",
    "while not os.path.exists(os.path.join(repo_root, '.git')):\n",
    "    # Move up one directory\n",
    "    parent_dir = os.path.dirname(repo_root)\n",
    "    if parent_dir == repo_root: # Reached filesystem root, .git not found\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the .git directory. \"\n",
    "            \"Please ensure you are running this code from within a Git repository.\"\n",
    "        )\n",
    "    repo_root = parent_dir\n",
    "\n",
    "if os.getcwd() != repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\") # Informative print for users\n",
    "\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Path to the data file, relative to the repository root.\n",
    "data_file_name = 'Customer_Purchasing_Behaviors.csv'\n",
    "data_file_path = os.path.join('src', 'data', data_file_name)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    print(f\"Successfully loaded '{data_file_name}'.\")\n",
    "    #print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{data_file_name}' was not found at '{data_file_path}'.\")\n",
    "    print(\"Please ensure it exists in the 'src/data/' folder relative to the repository root.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")\n",
    "\n",
    "# Create a copy for feature engineering to keep the original data safe\n",
    "df_eng = df.copy()\n",
    "print(\"Original DataFrame shape:\", df_eng.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956554f",
   "metadata": {},
   "source": [
    "**1. Handling Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0852f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame after handling 'region' ---\n",
      "   user_id region region_grouped  region_South  region_West\n",
      "0        1  North          North         False        False\n",
      "1        2  South          South          True        False\n",
      "2        3   West           West         False         True\n",
      "3        4   East          North         False        False\n",
      "4        5  North          North         False        False\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Group the underrepresented 'East' region to ensure model stability and prevent learning from statistical noise.\n",
    "df_eng['region_grouped'] = df_eng['region'].replace({'East': 'North'})\n",
    "\n",
    "# Convert categorical data into numerical format using One-Hot Encoding\n",
    "region_dummies = pd.get_dummies(df_eng['region_grouped'], prefix='region', drop_first=True)\n",
    "df_eng = pd.concat([df_eng, region_dummies], axis=1)\n",
    "\n",
    "print(\"\\n--- DataFrame after handling 'region' ---\")\n",
    "print(df_eng[['user_id', 'region', 'region_grouped', 'region_South', 'region_West']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ee7aa",
   "metadata": {},
   "source": [
    "**2. Creating Ratio-Based Features (Behavioral Insights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56d888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Newly created ratio features ---\n",
      "   user_id  spend_per_purchase  spend_to_income_ratio\n",
      "0        1           16.666667               0.004444\n",
      "1        2           19.444444               0.006364\n",
      "2        3           22.727273               0.007692\n",
      "3        4           15.000000               0.005000\n",
      "4        5           16.923077               0.004681\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Ratios normalize for effects like purchase frequency and provide deeper behavioral context.\n",
    "df_eng['spend_per_purchase'] = df_eng['purchase_amount'] / df_eng['purchase_frequency']\n",
    "df_eng['spend_to_income_ratio'] = df_eng['purchase_amount'] / df_eng['annual_income']\n",
    "\n",
    "print(\"\\n--- Newly created ratio features ---\")\n",
    "print(df_eng[['user_id', 'spend_per_purchase', 'spend_to_income_ratio']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd49f56",
   "metadata": {},
   "source": [
    "**3. Creating Demographic Tiers (Binning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599117f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Newly created demographic tiers ---\n",
      "   user_id  age    age_group  annual_income income_bracket\n",
      "0        1   25  Young_Adult          45000  Medium_Income\n",
      "1        2   34        Adult          55000  Medium_Income\n",
      "2        3   45  Middle_Aged          65000    High_Income\n",
      "3        4   22  Young_Adult          30000     Low_Income\n",
      "4        5   29  Young_Adult          47000  Medium_Income\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Converts continuous variables into interpretable categories for business analysis and segmentation.\n",
    "age_bins = [14, 30, 45, 60, 100]\n",
    "age_labels = ['Young_Adult', 'Adult', 'Middle_Aged', 'Senior']\n",
    "df_eng['age_group'] = pd.cut(df_eng['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "income_bins = [0, 45000, 90000, 150000]\n",
    "income_labels = ['Low_Income', 'Medium_Income', 'High_Income']\n",
    "df_eng['income_bracket'] = pd.cut(df_eng['annual_income'], bins=income_bins, labels=income_labels, right=False)\n",
    "\n",
    "print(\"\\n--- Newly created demographic tiers ---\")\n",
    "print(df_eng[['user_id', 'age', 'age_group', 'annual_income', 'income_bracket']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287e724",
   "metadata": {},
   "source": [
    "**4. Creating Composite Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07c16a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Newly created composite scores ---\n",
      "   user_id  customer_value_score  churn_risk_score\n",
      "0        1              0.136490          0.829060\n",
      "1        2              0.469039          0.470085\n",
      "2        3              0.716117          0.282051\n",
      "3        4              0.000000          1.000000\n",
      "4        5              0.182326          0.778205\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Combines multiple collinear features into single, powerful, and interpretable scores for value and risk.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df_eng[['purchase_amount', 'purchase_frequency', 'loyalty_score']])\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=['purchase_scaled', 'frequency_scaled', 'loyalty_scaled'])\n",
    "\n",
    "# Customer Value Score (weighted sum of key metrics)\n",
    "weights = {'monetary': 0.5, 'frequency': 0.25, 'loyalty': 0.25}\n",
    "df_eng['customer_value_score'] = (weights['monetary'] * df_scaled['purchase_scaled'] +\n",
    "                                  weights['frequency'] * df_scaled['frequency_scaled'] +\n",
    "                                  weights['loyalty'] * df_scaled['loyalty_scaled'])\n",
    "\n",
    "# Churn Risk Score (high for low loyalty and frequency)\n",
    "df_eng['churn_risk_score'] = (0.5 * (1 - df_scaled['loyalty_scaled']) +\n",
    "                              0.5 * (1 - df_scaled['frequency_scaled']))\n",
    "\n",
    "print(\"\\n--- Newly created composite scores ---\")\n",
    "print(df_eng[['user_id', 'customer_value_score', 'churn_risk_score']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967f164",
   "metadata": {},
   "source": [
    "**5. Creating Interaction and Segmentation Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85dffb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Binary Segmentation Flags ---\n",
      "   user_id  is_high_value  is_loyal  is_frequent  is_champion\n",
      "0        1          False     False        False        False\n",
      "1        2          False     False        False        False\n",
      "2        3          False     False        False        False\n",
      "3        4          False     False        False        False\n",
      "4        5          False     False        False        False\n",
      "Number of Champion Customers: 59\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Create binary flags for easy filtering and to identify high-value customer segments like 'Champions'.\n",
    "\n",
    "# Binary Segmentation Flags (based on top 25% percentile)\n",
    "high_value_threshold = df_eng['purchase_amount'].quantile(0.75)\n",
    "high_loyalty_threshold = df_eng['loyalty_score'].quantile(0.75)\n",
    "high_frequency_threshold = df_eng['purchase_frequency'].quantile(0.75)\n",
    "\n",
    "df_eng['is_high_value'] = (df_eng['purchase_amount'] > high_value_threshold)#.astype(int)\n",
    "df_eng['is_loyal'] = (df_eng['loyalty_score'] > high_loyalty_threshold)#.astype(int)\n",
    "df_eng['is_frequent'] = (df_eng['purchase_frequency'] > high_frequency_threshold)#.astype(int)\n",
    "df_eng['is_champion'] = (df_eng['is_high_value'] * df_eng['is_loyal'] * df_eng['is_frequent'])#.astype(int)\n",
    "\n",
    "print(\"\\n--- Binary Segmentation Flags ---\")\n",
    "print(df_eng[['user_id', 'is_high_value', 'is_loyal', 'is_frequent', 'is_champion']].head())\n",
    "print(f\"Number of Champion Customers: {df_eng['is_champion'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f23cc3",
   "metadata": {},
   "source": [
    "**6. Creating Statistical and Business-Savvy Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bc616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Growth Potential & Percentile Scores ---\n",
      "    user_id  annual_income  purchase_amount  growth_potential_score\n",
      "4         5          47000              220                     100\n",
      "30       31          55000              350                     100\n",
      "1         2          55000              350                     100\n",
      "45       46          47000              220                     100\n",
      "22       23          63000              460                      88\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Create normalized ranks and business-oriented scores like 'Growth Potential'.\n",
    "\n",
    "# Percentile Ranks\n",
    "df_eng['income_percentile'] = df_eng['annual_income'].rank(pct=True)\n",
    "df_eng['spending_percentile'] = df_eng['purchase_amount'].rank(pct=True)\n",
    "\n",
    "# Growth Potential Score (High Income, Relatively Low Spending)\n",
    "df_eng['growth_potential_score'] = df_eng['income_percentile'] - df_eng['spending_percentile']\n",
    "\n",
    "# Scaling to range [0-100] # NOTE TO BE DELETED: Added after the meeting\n",
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "# Apply scaling\n",
    "df_eng['growth_potential_score'] = scaler.fit_transform(df_eng[['growth_potential_score']]).astype(int)\n",
    "\n",
    "print(\"\\n--- Growth Potential & Percentile Scores ---\")\n",
    "print(df_eng.sort_values('growth_potential_score', ascending=False)[['user_id', 'annual_income', 'purchase_amount', 'growth_potential_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cd7e2",
   "metadata": {},
   "source": [
    "**7. Finalizing the Model-Ready DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2b21b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL MODEL-READY DATAFRAME ---\n",
      "Shape: (238, 11)\n",
      "Columns: ['user_id', 'customer_value_score', 'churn_risk_score', 'growth_potential_score', 'spend_per_purchase', 'income_to_spend_ratio', 'is_champion', 'age', 'annual_income', 'region_South', 'region_West']\n",
      "   user_id  customer_value_score  churn_risk_score  growth_potential_score  \\\n",
      "0        1              0.136490          0.829060                0.067227   \n",
      "1        2              0.469039          0.470085                0.126050   \n",
      "2        3              0.716117          0.282051                0.029412   \n",
      "3        4              0.000000          1.000000                0.000000   \n",
      "4        5              0.182326          0.778205                0.126050   \n",
      "\n",
      "   spend_per_purchase  income_to_spend_ratio  is_champion  age  annual_income  \\\n",
      "0           16.666667               0.004444        False   25          45000   \n",
      "1           19.444444               0.006364        False   34          55000   \n",
      "2           22.727273               0.007692        False   45          65000   \n",
      "3           15.000000               0.005000        False   22          30000   \n",
      "4           16.923077               0.004681        False   29          47000   \n",
      "\n",
      "   region_South  region_West  \n",
      "0         False        False  \n",
      "1          True        False  \n",
      "2         False         True  \n",
      "3         False        False  \n",
      "4         False        False  \n"
     ]
    }
   ],
   "source": [
    "# Rationale: Create a final, clean DataFrame containing only the identifier and the best engineered features for modeling.\n",
    "# This prevents data leakage and removes redundant columns.\n",
    "\n",
    "features_for_modeling = [\n",
    "    'user_id',\n",
    "    # --- Core Scores ---\n",
    "    'customer_value_score',\n",
    "    'churn_risk_score',\n",
    "    'growth_potential_score',\n",
    "    # --- Behavioral Ratios ---\n",
    "    'spend_per_purchase',\n",
    "    'income_to_spend_ratio',\n",
    "    # --- Key Segments/Flags ---\n",
    "    'is_champion',\n",
    "    # --- Raw Demographics (for direct use) ---\n",
    "    'age',\n",
    "    'annual_income'\n",
    "]\n",
    "\n",
    "# Dynamically add the one-hot encoded region columns to the list\n",
    "final_feature_list = features_for_modeling + list(region_dummies.columns)\n",
    "\n",
    "df_model_ready = df_eng[final_feature_list].copy()\n",
    "\n",
    "print(\"\\n--- FINAL MODEL-READY DATAFRAME ---\")\n",
    "print(\"Shape:\", df_model_ready.shape)\n",
    "print(\"Columns:\", df_model_ready.columns.tolist())\n",
    "print(df_model_ready.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e5e233",
   "metadata": {},
   "source": [
    "**The following is an analysis of the post that Vinod posted in Slack group chat:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcb05a",
   "metadata": {},
   "source": [
    "**7. Segmentation features - (is_high_value, is_loyal, is_frequent, customer_tier)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_tier. Would this be market segmentations? Would it come out from clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb4f08",
   "metadata": {},
   "source": [
    "**8. Demographic behavioural interaction - (young_high_spender, senior_loyal, income_age_segment, etc..)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic behavioural interaction. Would this be market segmentations? Would it come out from clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b0eb6",
   "metadata": {},
   "source": [
    "**9. Statistical Features - (frequency_percentile, is_outlier_spender, loyalty_deviation, etc..)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c747da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0-25%\n",
       "1    25-50%\n",
       "2    50-75%\n",
       "3     0-25%\n",
       "4     0-25%\n",
       "Name: frequency_percentile, dtype: category\n",
       "Categories (4, object): ['0-25%' < '25-50%' < '50-75%' < '75-100%']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE TO BE DELETED: Added after the meeting\n",
    "# Define labels for each quantile\n",
    "labels = ['0-25%', '25-50%', '50-75%', '75-100%']\n",
    "# frequency_percentile\n",
    "df['frequency_percentile'] = pd.qcut(df['purchase_frequency'], q=4, labels=labels)\n",
    "\n",
    "df['frequency_percentile'].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
