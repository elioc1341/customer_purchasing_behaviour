{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67873f7a",
   "metadata": {},
   "source": [
    "**0. Load and Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e18bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# set visulization style\n",
    "sns.set_style(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b6b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\The Winner\\DSI\\customer_purchasing_behaviour\n",
      "Successfully loaded 'Customer_Purchasing_Behaviors.csv'.\n",
      "Original DataFrame shape: (238, 7)\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure consistent working directory for data loading ---\n",
    "# This block dynamically sets the current working directory to the Git repository root.\n",
    "# This makes data paths reliable for all collaborators, regardless of where they open the notebook.\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_root = current_dir\n",
    "while not os.path.exists(os.path.join(repo_root, '.git')):\n",
    "    # Move up one directory\n",
    "    parent_dir = os.path.dirname(repo_root)\n",
    "    if parent_dir == repo_root: # Reached filesystem root, .git not found\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the .git directory. \"\n",
    "            \"Please ensure you are running this code from within a Git repository.\"\n",
    "        )\n",
    "    repo_root = parent_dir\n",
    "\n",
    "if os.getcwd() != repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\") # Informative print for users\n",
    "\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Path to the data file, relative to the repository root.\n",
    "data_file_name = 'Customer_Purchasing_Behaviors.csv'\n",
    "data_file_path = os.path.join('src', 'data', data_file_name)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    print(f\"Successfully loaded '{data_file_name}'.\")\n",
    "    #print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{data_file_name}' was not found at '{data_file_path}'.\")\n",
    "    print(\"Please ensure it exists in the 'src/data/' folder relative to the repository root.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")\n",
    "\n",
    "# Create a copy for feature engineering to keep the original data safe\n",
    "df_eng = df.copy()\n",
    "print(\"Original DataFrame shape:\", df_eng.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956554f",
   "metadata": {},
   "source": [
    "**1. Handling Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0852f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame after handling 'region' ---\n",
      "   user_id region region_grouped  region_South  region_West\n",
      "0        1  North          North         False        False\n",
      "1        2  South          South          True        False\n",
      "2        3   West           West         False         True\n",
      "3        4   East          North         False        False\n",
      "4        5  North          North         False        False\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Group the underrepresented 'East' region to ensure model stability and prevent learning from statistical noise.\n",
    "df_eng['region_grouped'] = df_eng['region'].replace({'East': 'North'})\n",
    "\n",
    "# Convert categorical data into numerical format using One-Hot Encoding\n",
    "region_dummies = pd.get_dummies(df_eng['region_grouped'], prefix='region', drop_first=True)\n",
    "df_eng = pd.concat([df_eng, region_dummies], axis=1)\n",
    "\n",
    "print(\"\\n--- DataFrame after handling 'region' ---\")\n",
    "print(df_eng[['user_id', 'region', 'region_grouped', 'region_South', 'region_West']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ee7aa",
   "metadata": {},
   "source": [
    "**2. Creating Ratio-Based Features (Behavioral Insights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56d888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Newly created ratio features ---\n",
      "   user_id  spend_per_purchase  income_to_spend_ratio\n",
      "0        1           16.666667               0.004444\n",
      "1        2           19.444444               0.006364\n",
      "2        3           22.727273               0.007692\n",
      "3        4           15.000000               0.005000\n",
      "4        5           16.923077               0.004681\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Ratios normalize for effects like purchase frequency and provide deeper behavioral context.\n",
    "df_eng['spend_per_purchase'] = df_eng['purchase_amount'] / df_eng['purchase_frequency']\n",
    "df_eng['income_to_spend_ratio'] = df_eng['purchase_amount'] / df_eng['annual_income']\n",
    "\n",
    "print(\"\\n--- Newly created ratio features ---\")\n",
    "print(df_eng[['user_id', 'spend_per_purchase', 'income_to_spend_ratio']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd49f56",
   "metadata": {},
   "source": [
    "**3. Creating Demographic Tiers (Binning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599117f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Newly created demographic tiers ---\n",
      "   user_id  age    age_group  annual_income income_bracket\n",
      "0        1   25  Young Adult          45000  Medium Income\n",
      "1        2   34        Adult          55000  Medium Income\n",
      "2        3   45  Middle-Aged          65000    High Income\n",
      "3        4   22  Young Adult          30000     Low Income\n",
      "4        5   29  Young Adult          47000  Medium Income\n"
     ]
    }
   ],
   "source": [
    "# Rationale: Converts continuous variables into interpretable categories for business analysis and segmentation.\n",
    "age_bins = [18, 30, 45, 60, 100]\n",
    "age_labels = ['Young Adult', 'Adult', 'Middle-Aged', 'Senior']\n",
    "df_eng['age_group'] = pd.cut(df_eng['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "income_bins = [0, 45000, 65000, 150000]\n",
    "income_labels = ['Low Income', 'Medium Income', 'High Income']\n",
    "df_eng['income_bracket'] = pd.cut(df_eng['annual_income'], bins=income_bins, labels=income_labels, right=False)\n",
    "\n",
    "print(\"\\n--- Newly created demographic tiers ---\")\n",
    "print(df_eng[['user_id', 'age', 'age_group', 'annual_income', 'income_bracket']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287e724",
   "metadata": {},
   "source": [
    "**4. Creating Composite Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c16a8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rationale: Combines multiple collinear features into single, powerful, and interpretable scores for value and risk.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mMinMaxScaler\u001b[49m()\n\u001b[0;32m      3\u001b[0m scaled_features \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df_eng[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloyalty_score\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      4\u001b[0m df_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaled_features, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_scaled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency_scaled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloyalty_scaled\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# Rationale: Combines multiple collinear features into single, powerful, and interpretable scores for value and risk.\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df_eng[['purchase_amount', 'purchase_frequency', 'loyalty_score']])\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=['purchase_scaled', 'frequency_scaled', 'loyalty_scaled'])\n",
    "\n",
    "# Customer Value Score (weighted sum of key metrics)\n",
    "weights = {'monetary': 0.5, 'frequency': 0.25, 'loyalty': 0.25}\n",
    "df_eng['customer_value_score'] = (weights['monetary'] * df_scaled['purchase_scaled'] +\n",
    "                                  weights['frequency'] * df_scaled['frequency_scaled'] +\n",
    "                                  weights['loyalty'] * df_scaled['loyalty_scaled'])\n",
    "\n",
    "# Churn Risk Score (high for low loyalty and frequency)\n",
    "df_eng['churn_risk_score'] = (0.5 * (1 - df_scaled['loyalty_scaled']) +\n",
    "                              0.5 * (1 - df_scaled['frequency_scaled']))\n",
    "\n",
    "print(\"\\n--- Newly created composite scores ---\")\n",
    "print(df_eng[['user_id', 'customer_value_score', 'churn_risk_score']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967f164",
   "metadata": {},
   "source": [
    "**5. Creating Interaction and Segmentation Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dffb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rationale: Create binary flags for easy filtering and to identify high-value customer segments like 'Champions'.\n",
    "\n",
    "# Binary Segmentation Flags (based on top 25% percentile)\n",
    "high_value_threshold = df_eng['purchase_amount'].quantile(0.75)\n",
    "high_loyalty_threshold = df_eng['loyalty_score'].quantile(0.75)\n",
    "high_frequency_threshold = df_eng['purchase_frequency'].quantile(0.75)\n",
    "\n",
    "df_eng['is_high_value'] = (df_eng['purchase_amount'] > high_value_threshold).astype(int)\n",
    "df_eng['is_loyal'] = (df_eng['loyalty_score'] > high_loyalty_threshold).astype(int)\n",
    "df_eng['is_frequent'] = (df_eng['purchase_frequency'] > high_frequency_threshold).astype(int)\n",
    "df_eng['is_champion'] = (df_eng['is_high_value'] * df_eng['is_loyal'] * df_eng['is_frequent']).astype(int)\n",
    "\n",
    "print(\"\\n--- Binary Segmentation Flags ---\")\n",
    "print(df_eng[['user_id', 'is_high_value', 'is_loyal', 'is_frequent', 'is_champion']].head())\n",
    "print(f\"Number of Champion Customers: {df_eng['is_champion'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f23cc3",
   "metadata": {},
   "source": [
    "**6. Creating Statistical and Business-Savvy Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rationale: Create normalized ranks and business-oriented scores like 'Growth Potential'.\n",
    "\n",
    "# Percentile Ranks\n",
    "df_eng['income_percentile'] = df_eng['annual_income'].rank(pct=True)\n",
    "df_eng['spending_percentile'] = df_eng['purchase_amount'].rank(pct=True)\n",
    "\n",
    "# Growth Potential Score (High Income, Relatively Low Spending)\n",
    "df_eng['growth_potential_score'] = df_eng['income_percentile'] - df_eng['spending_percentile']\n",
    "\n",
    "print(\"\\n--- Growth Potential & Percentile Scores ---\")\n",
    "print(df_eng.sort_values('growth_potential_score', ascending=False)[['user_id', 'annual_income', 'purchase_amount', 'growth_potential_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cd7e2",
   "metadata": {},
   "source": [
    "**7. Finalizing the Model-Ready DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b21b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rationale: Create a final, clean DataFrame containing only the identifier and the best engineered features for modeling.\n",
    "# This prevents data leakage and removes redundant columns.\n",
    "\n",
    "features_for_modeling = [\n",
    "    'user_id',\n",
    "    # --- Core Scores ---\n",
    "    'customer_value_score',\n",
    "    'churn_risk_score',\n",
    "    'growth_potential_score',\n",
    "    # --- Behavioral Ratios ---\n",
    "    'spend_per_purchase',\n",
    "    'income_to_spend_ratio',\n",
    "    # --- Key Segments/Flags ---\n",
    "    'is_champion',\n",
    "    # --- Raw Demographics (for direct use) ---\n",
    "    'age',\n",
    "    'annual_income'\n",
    "]\n",
    "\n",
    "# Dynamically add the one-hot encoded region columns to the list\n",
    "final_feature_list = features_for_modeling + list(region_dummies.columns)\n",
    "\n",
    "df_model_ready = df_eng[final_feature_list].copy()\n",
    "\n",
    "print(\"\\n--- FINAL MODEL-READY DATAFRAME ---\")\n",
    "print(\"Shape:\", df_model_ready.shape)\n",
    "print(\"Columns:\", df_model_ready.columns.tolist())\n",
    "print(df_model_ready.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcb05a",
   "metadata": {},
   "source": [
    "**7. Segmentation features - (is_high_value, is_loyal, is_frequent, customer_tier)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26db1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_high_value\n",
    "df_eng['spender'] = ['is_high_value' if x >= df['purchase_amount'].quantile(0.75) else 'high_value_in_progress' \n",
    "                 for x in df['purchase_amount']]\n",
    "#df['spender'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_loyal\n",
    "df['loyal'] = ['is_loyal' if x >= df['loyalty_score'].quantile(0.75) else 'loyalty_in_progress' \n",
    "                 for x in df['loyalty_score']]\n",
    "#df['loyal'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf52686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_frequent\n",
    "df['frequent'] = ['is_frequent' if x >= df['purchase_frequency'].quantile(0.75) else 'frequency_in_progress' \n",
    "                 for x in df['purchase_frequency']]\n",
    "#df['frequent'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_tier. Would this be market segmentations? Would it come out from clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb4f08",
   "metadata": {},
   "source": [
    "**8. Demographic behavioural interaction - (young_high_spender, senior_loyal, income_age_segment, etc..)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic behavioural interaction. Would this be market segmentations? Would it come out from clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b0eb6",
   "metadata": {},
   "source": [
    "**9. Statistical Features - (frequency_percentile, is_outlier_spender, loyalty_deviation, etc..)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c747da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency_percentile\n",
    "df['frequency_percentile'] = ['is_frequent' if x >= df['purchase_frequency'].quantile(0.75) else 'frequency_in_progress' \n",
    "                 for x in df['purchase_frequency']]\n",
    "df['frequency_percentile'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b30094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({'purchase_amount': [120, 250, 75, 300, 180, 220, 90, 310]})\n",
    "\n",
    "# Define labels for each quantile\n",
    "labels = ['0-25%', '25-50%', '50-75%', '75-100%']\n",
    "\n",
    "# Create 'spender_group' column with labels\n",
    "df['spender_group'] = pd.qcut(df['purchase_amount'], q=4, labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
