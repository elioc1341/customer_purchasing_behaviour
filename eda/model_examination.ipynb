{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe21714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# set visulization style\n",
    "sns.set_style(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa76e714",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find the .git directory. Please ensure you are running this code from within a Git repository.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     parent_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(repo_root)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parent_dir \u001b[38;5;241m==\u001b[39m repo_root: \u001b[38;5;66;03m# Reached filesystem root, .git not found\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the .git directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are running this code from within a Git repository.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m         )\n\u001b[0;32m     15\u001b[0m     repo_root \u001b[38;5;241m=\u001b[39m parent_dir\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Change the current working directory if it's not already the repo root\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find the .git directory. Please ensure you are running this code from within a Git repository."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ensure consistent working directory for data loading ---\n",
    "# This block dynamically sets the current working directory to the Git repository root.\n",
    "# This makes data paths reliable for all collaborators, regardless of where they open the notebook.\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_root = current_dir\n",
    "while not os.path.exists(os.path.join(repo_root, '.git')):\n",
    "    # Move up one directory\n",
    "    parent_dir = os.path.dirname(repo_root)\n",
    "    if parent_dir == repo_root: # Reached filesystem root, .git not found\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the .git directory. \"\n",
    "            \"Please ensure you are running this code from within a Git repository.\"\n",
    "        )\n",
    "    repo_root = parent_dir\n",
    "\n",
    "# Change the current working directory if it's not already the repo root\n",
    "if os.getcwd() != repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\") # Informative print for users\n",
    "\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Path to the data file, relative to the repository root.\n",
    "data_file_name = 'Customer Purchasing Behaviors.csv'\n",
    "data_file_path = os.path.join('data', 'raw', data_file_name)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    print(f\"Successfully loaded '{data_file_name}'.\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{data_file_name}' was not found at '{data_file_path}'.\")\n",
    "    print(\"Please ensure it exists in the 'src/data/' folder relative to the repository root.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5d3e7",
   "metadata": {},
   "source": [
    "Initial observations:\n",
    "-East region heavily under represented\n",
    "-no data for incomes ~$32-42K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData Info\")\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c42dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values\")\n",
    "print(df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932828d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStatistical Summary\")\n",
    "print(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d806e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ea3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "fig.suptitle('Distribution of Numerical Features', fontsize=20)\n",
    "\n",
    "# Age Distribution\n",
    "sns.histplot(df['age'], bins=20, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "\n",
    "# Annual Income Distribution\n",
    "sns.histplot(df['annual_income'], bins=20, kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Annual Income Distribution')\n",
    "\n",
    "# Purchase Amount Distribution\n",
    "sns.histplot(df['purchase_amount'], bins=20, kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Total Purchase Amount Distribution')\n",
    "\n",
    "# Loyalty Score Distribution\n",
    "sns.histplot(df['loyalty_score'], bins=20, kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Loyalty Score Distribution')\n",
    "\n",
    "# Purchase Frequency Distribution\n",
    "sns.histplot(df['purchase_frequency'], bins=20, kde=True, ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Purchase Frequency Distribution')\n",
    "\n",
    "# Regional Distribution\n",
    "sns.countplot(data=df, x='region', ax=axes[2, 1], hue='region', palette='viridis')\n",
    "axes[2, 1].set_title('Customer Distribution by Region')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f9277",
   "metadata": {},
   "source": [
    "## Understanding Feature Distributions by Univariate Analysis\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "*   **Multimodal Distributions:** Several features, notably `age`, `annual_income`, and `purchase_amount`, exhibit multimodal distributions (having multiple peaks). This is a strong indicator that there are likely 2-3 distinct subgroups of customers within the dataset. For instance, the peaks in `age` around 30 and 50 could represent different life-stage segments.\n",
    "\n",
    "*   **Narrow Ranges:** The `loyalty_score` (3.0 to 9.5) and `purchase_frequency` (10 to 28) exist within surprisingly narrow bands. This is uncharacteristic of real-world data and points towards the dataset being synthetic. \n",
    "\n",
    "*   **Categorical Feature:** The `region` feature is balanced across North, South, and West but the East is highly underrepresented, any predictions for customers in the \"East\" region are likely to have high uncertainty. Small sample size means this average could be wildly inaccurate and not representative of the true \"East\" population. \n",
    "\n",
    "**Implication for Modeling:**\n",
    "\n",
    "The clear presence of multiple modes in the data strongly suggests that **unsupervised clustering (e.g., K-Means)** will be a highly effective technique for identifying these inherent customer segments. The East region being highly under represented, we could either acknowledge it proceed with caution or we could create a new feature `North_East`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the dataframe\n",
    "df_working = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e728cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_working.drop(['user_id', 'region'], axis=1).corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ae8ab",
   "metadata": {},
   "source": [
    "## Quantifying Linear Relationships through Bivariate Analysis\n",
    "\n",
    "**Key Observations**\n",
    "\n",
    "*   The heatmap shows correlation coefficients that are consistently **above 0.97** for all pairs of numerical features. This is an extreme level of multicollinearity, \n",
    "indicating that the features are not just related, but are nearly perfect linear combinations of each other. A key feature of synthetic data. \n",
    "\n",
    "\n",
    "**Implications for Modeling & Our Strategic Response:**\n",
    "\n",
    "The extreme multicollinearity is the single most important factor guiding our modeling strategy. It dictates which models are appropriate and how we must interpret their results.\n",
    "\n",
    "*   **For Linear Models (e.g., Linear/Logistic Regression): Complete Instability.**\n",
    "    *   **Problem:** These models will be highly unstable. Multicollinearity makes it impossible for the model to decide how to assign credit. Think of it like two people writing a report together; you can't tell who wrote 90% and who wrote 10%. The model's coefficients will be mathematically unreliable and completely uninterpretable.\n",
    "    *   **Our Action:** We will avoid using linear models for our final predictive task. We may, however, train one briefly to *demonstrate* this instability, showcasing our diagnostic skills.\n",
    "\n",
    "*   **For Tree-Based Models (e.g., XGBoost): Predictive Accuracy vs. Interprtation**\n",
    "    *   **Good News:** These models are robust in their predictive power. They will still make accurate predictions because they can simply pick one of the correlated features (e.g., `annual_income`) at each split and effectively learn the underlying patterns.\n",
    "    *   **The Challenge:** The *interpretation* of feature importance (like SHAP values) requires nuance. The model will likely attribute all importance to the one feature it picked and assign zero importance to its correlated partners.\n",
    "    *   **Our Interpretation Strategy:** We will interpret this correctly: not as \"`annual_income` is the only important feature,\" but as \"**`annual_income` is acting as the chosen representative for the entire 'customer value' signal**\" that is shared across the correlated group.\n",
    "\n",
    "*   **Our Overarching Action Plan: Disciplined Feature Management.**\n",
    "    *   **Mandatory Feature Selection for Prediction:** For any *interpretable predictive model*, we will perform disciplined feature selection. We will select a minimal, non-redundant set of features (e.g., `age`, `annual_income`) that are most business-relevant to serve as inputs.\n",
    "    *   **Leverage Redundancy for Segmentation:** For unsupervised clustering (K-Means), we can use all features. Here, the redundant information can actually help reinforce the cluster structures, making the segments more distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to see all bivariate relationships\n",
    "# We use a sample to make it render faster if needed, but 238 rows is fine.\n",
    "pair_plot = sns.pairplot(df_working, hue='region', palette='viridis', corner=True)\n",
    "pair_plot.fig.suptitle('Pairwise Relationships by Region', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038a896",
   "metadata": {},
   "source": [
    "## Pairwise Relationships\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "*   **Confirmation of Linearity:** The scatter plots confirm the extreme multicollinearity. The relationships are not just general trends; they form near-perfect straight lines. This is the most compelling visual evidence of the dataset's synthetic, formulaic origin.\n",
    "\n",
    "*   **No Separation Among Dominant Regions:** The `hue` coloring shows that data points for the well-represented regions (**North, South, and West**) are completely mixed. This indicates that for ~90% of our customers, their region is not a meaningful differentiator of their behavior.\n",
    "\n",
    "*   **No Obvious Non-Linearity:** There are no curved or complex patterns. The underlying relationships in this dataset are overwhelmingly linear.\n",
    "\n",
    "**Implication for Modeling:**\n",
    "\n",
    "*   **Low Predictive Power of `region`:** The feature has very little predictive power for the majority of the data.\n",
    "*   **Handling the \"East\" Category:** This plot provides the visual justification for our strategy to handle the \"East\" category. We cannot trust the model to learn meaningful patterns from such a small sample. Therefore, grouping \"East\" into a combined category is the most robust approach to prevent the model from learning spurious, noise-driven rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf8f7d",
   "metadata": {},
   "source": [
    "### EDA Summary\n",
    "\n",
    "**Our comprehensive Exploratory Data Analysis (EDA) has revealed that the dataset is highly synthetic and defined by two critical flaws:**\n",
    "\n",
    "1. Extreme Multicollinearity: All numerical features are almost perfectly correlated (>0.97), making them informationally redundant.\n",
    "2. Severe Class Imbalance: The \"East\" region is drastically underrepresented, making any statistical conclusions about it unreliable.\n",
    "\n",
    "Instead of treating these as blockers, we are making them the centerpiece of our project. Our objective has shifted from simple prediction to a more sophisticated goal: to showcase a robust, end-to-end ML methodology for handling compromised, real-world-like data.\n",
    "\n",
    "**Our possible action plan is:**\n",
    "\n",
    "**Leverage the Data's Strengths:** Use the clear, linear structure for a powerful customer segmentation model using K-Means.\n",
    "\n",
    "**Mitigate the Flaws for Prediction:** Use tree-based models with disciplined feature selection to create stable predictive models, avoiding the instability of linear approaches.\n",
    "\n",
    "**Demonstrate Nuanced Interpretation:** Deliver a cautious interpretation of model results (especially feature importance), explaining how multicollinearity impacts them.\n",
    "\n",
    "**Deliver a Proof-of-Concept:** Package our entire workflow—from diagnostics to deployment—into a functional Tableau or Streamlit dashboard that proves the value of a well-designed customer intelligence platform, even when built on imperfect data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79541ef",
   "metadata": {},
   "source": [
    "**EXTRA - TEMPORARY LOCATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df[['age', 'annual_income', 'loyalty_score','puchase_frequency']]\n",
    "feature_names = df_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f42a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the relationships to try to obtain inputs to decide what non-linear transformation to apply\n",
    "\n",
    "def plot_feature_pairs(data, feature_names, color_labels=None, title_prefix=''):\n",
    "    \"\"\"\n",
    "    Helper function to create scatter plots for all possible pairs of features.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the features to be plotted.\n",
    "    - feature_names: List of feature names to be used in plotting.\n",
    "    - color_labels: Optional. Cluster or class labels to color the scatter plots.\n",
    "    - title_prefix: Optional. Prefix for plot titles to distinguish between different sets of plots.\n",
    "    \"\"\"\n",
    "    # Create a figure for the scatter plots\n",
    "    plt.figure(figsize=(60, 60))\n",
    "    \n",
    "    # Counter for subplot index\n",
    "    plot_number = 1\n",
    "    \n",
    "    # Loop through each pair of features\n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i + 1, len(feature_names)):\n",
    "            plt.subplot(len(feature_names)-1, len(feature_names)-1, plot_number)\n",
    "            \n",
    "            # Scatter plot colored by labels if provided\n",
    "            if color_labels is not None:\n",
    "                plt.scatter(data[feature_names[i]], data[feature_names[j]], \n",
    "                            c=color_labels, cmap='viridis', alpha=0.7)\n",
    "            else:\n",
    "                plt.scatter(data[feature_names[i]], data[feature_names[j]], alpha=0.7)\n",
    "            \n",
    "            plt.xlabel(feature_names[i])\n",
    "            plt.ylabel(feature_names[j])\n",
    "            plt.title(f'{title_prefix}{feature_names[i]} vs {feature_names[j]}')\n",
    "            \n",
    "            # Increment the plot number\n",
    "            plot_number += 1\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = fires_dt.columns\n",
    "\n",
    "# Use the helper function to plot scatter plots without coloring by cluster labels\n",
    "plot_feature_pairs(df, feature_names, title_prefix='Original Data: ')                          #________P_L_O_T________#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
