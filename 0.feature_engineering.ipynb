{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67873f7a",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e18bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# set visulization style\n",
    "sns.set_style(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7b6b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'Customer Purchasing Behaviors.csv'.\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure consistent working directory for data loading ---\n",
    "# This block dynamically sets the current working directory to the Git repository root.\n",
    "# This makes data paths reliable for all collaborators, regardless of where they open the notebook.\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_root = current_dir\n",
    "while not os.path.exists(os.path.join(repo_root, '.git')):\n",
    "    # Move up one directory\n",
    "    parent_dir = os.path.dirname(repo_root)\n",
    "    if parent_dir == repo_root: # Reached filesystem root, .git not found\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the .git directory. \"\n",
    "            \"Please ensure you are running this code from within a Git repository.\"\n",
    "        )\n",
    "    repo_root = parent_dir\n",
    "\n",
    "if os.getcwd() != repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\") # Informative print for users\n",
    "\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Path to the data file, relative to the repository root.\n",
    "data_file_name = 'Customer Purchasing Behaviors.csv'\n",
    "data_file_path = os.path.join('src', 'data', data_file_name)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    print(f\"Successfully loaded '{data_file_name}'.\")\n",
    "    #print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{data_file_name}' was not found at '{data_file_path}'.\")\n",
    "    print(\"Please ensure it exists in the 'src/data/' folder relative to the repository root.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a15d1",
   "metadata": {},
   "source": [
    "**0. Load and Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e80d189e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'customer_data.csv' not found. Please check the file path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'customer_data.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Create a dummy dataframe to allow the rest of the script to run for demonstration\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m238\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m88\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannual_income\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30000\u001b[39m, \u001b[38;5;241m100000\u001b[39m, \u001b[38;5;241m295\u001b[39m), \n\u001b[0;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m650\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloyalty_score\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9.5\u001b[39m, \u001b[38;5;241m238\u001b[39m),\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSouth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEast\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m59\u001b[39m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSouth\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m29\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m238\u001b[39m]}\n\u001b[1;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA dummy dataframe has been created for demonstration purposes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a copy for feature engineering to keep the original data safe\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Ensure 'customer_data.csv' is in the same directory or provide the full path.\n",
    "try:\n",
    "    df = pd.read_csv('customer_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'customer_data.csv' not found. Please check the file path.\")\n",
    "    # Create a dummy dataframe to allow the rest of the script to run for demonstration\n",
    "    data = {'user_id': range(238), 'age': range(18, 88, 1), 'annual_income': range(30000, 100000, 295), \n",
    "            'purchase_amount': range(150, 650, 2), 'loyalty_score': np.linspace(3, 9.5, 238),\n",
    "            'region': ['North', 'South', 'West', 'East'] * 59 + ['North', 'South'],\n",
    "            'purchase_frequency': range(10, 29, 1)[0:238]}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"A dummy dataframe has been created for demonstration purposes.\")\n",
    "\n",
    "\n",
    "# Create a copy for feature engineering to keep the original data safe\n",
    "df_eng = df.copy()\n",
    "print(\"Original DataFrame shape:\", df_eng.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956554f",
   "metadata": {},
   "source": [
    "**1. Handling Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0852f5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rationale: Group the underrepresented 'East' region to ensure model stability and prevent learning from statistical noise.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion_grouped\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_eng\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEast\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorth\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert categorical data into numerical format using One-Hot Encoding\u001b[39;00m\n\u001b[0;32m      5\u001b[0m region_dummies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion_grouped\u001b[39m\u001b[38;5;124m'\u001b[39m], prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m, drop_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_eng' is not defined"
     ]
    }
   ],
   "source": [
    "# Rationale: Group the underrepresented 'East' region to ensure model stability and prevent learning from statistical noise.\n",
    "df_eng['region_grouped'] = df_eng['region'].replace({'East': 'North'})\n",
    "\n",
    "# Convert categorical data into numerical format using One-Hot Encoding\n",
    "region_dummies = pd.get_dummies(df_eng['region_grouped'], prefix='region', drop_first=True)\n",
    "df_eng = pd.concat([df_eng, region_dummies], axis=1)\n",
    "\n",
    "print(\"\\n--- DataFrame after handling 'region' ---\")\n",
    "print(df_eng[['user_id', 'region', 'region_grouped', 'region_South', 'region_West']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ee7aa",
   "metadata": {},
   "source": [
    "**2. Creating Ratio-Based Features (Behavioral Insights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f56d888e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rationale: Ratios normalize for effects like purchase frequency and provide deeper behavioral context.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspend_per_purchase\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_eng\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincome_to_spend_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannual_income\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Newly created ratio features ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_eng' is not defined"
     ]
    }
   ],
   "source": [
    "# Rationale: Ratios normalize for effects like purchase frequency and provide deeper behavioral context.\n",
    "df_eng['spend_per_purchase'] = df_eng['purchase_amount'] / df_eng['purchase_frequency']\n",
    "df_eng['income_to_spend_ratio'] = df_eng['purchase_amount'] / df_eng['annual_income']\n",
    "\n",
    "print(\"\\n--- Newly created ratio features ---\")\n",
    "print(df_eng[['user_id', 'spend_per_purchase', 'income_to_spend_ratio']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd49f56",
   "metadata": {},
   "source": [
    "**3. Creating Demographic Tiers (Binning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "599117f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m age_bins \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m45\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m      3\u001b[0m age_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYoung Adult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMiddle-Aged\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSenior\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_group\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(\u001b[43mdf_eng\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39mage_bins, labels\u001b[38;5;241m=\u001b[39mage_labels, right\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m income_bins \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m45000\u001b[39m, \u001b[38;5;241m65000\u001b[39m, \u001b[38;5;241m150000\u001b[39m]\n\u001b[0;32m      7\u001b[0m income_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow Income\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedium Income\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh Income\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_eng' is not defined"
     ]
    }
   ],
   "source": [
    "# Rationale: Converts continuous variables into interpretable categories for business analysis and segmentation.\n",
    "age_bins = [18, 30, 45, 60, 100]\n",
    "age_labels = ['Young Adult', 'Adult', 'Middle-Aged', 'Senior']\n",
    "df_eng['age_group'] = pd.cut(df_eng['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "income_bins = [0, 45000, 65000, 150000]\n",
    "income_labels = ['Low Income', 'Medium Income', 'High Income']\n",
    "df_eng['income_bracket'] = pd.cut(df_eng['annual_income'], bins=income_bins, labels=income_labels, right=False)\n",
    "\n",
    "print(\"\\n--- Newly created demographic tiers ---\")\n",
    "print(df_eng[['user_id', 'age', 'age_group', 'annual_income', 'income_bracket']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287e724",
   "metadata": {},
   "source": [
    "**4. Creating Composite Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07c16a8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rationale: Combines multiple collinear features into single, powerful, and interpretable scores for value and risk.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mMinMaxScaler\u001b[49m()\n\u001b[0;32m      3\u001b[0m scaled_features \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df_eng[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloyalty_score\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      4\u001b[0m df_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaled_features, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_scaled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency_scaled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloyalty_scaled\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# Rationale: Combines multiple collinear features into single, powerful, and interpretable scores for value and risk.\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df_eng[['purchase_amount', 'purchase_frequency', 'loyalty_score']])\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=['purchase_scaled', 'frequency_scaled', 'loyalty_scaled'])\n",
    "\n",
    "# Customer Value Score (weighted sum of key metrics)\n",
    "weights = {'monetary': 0.5, 'frequency': 0.25, 'loyalty': 0.25}\n",
    "df_eng['customer_value_score'] = (weights['monetary'] * df_scaled['purchase_scaled'] +\n",
    "                                  weights['frequency'] * df_scaled['frequency_scaled'] +\n",
    "                                  weights['loyalty'] * df_scaled['loyalty_scaled'])\n",
    "\n",
    "# Churn Risk Score (high for low loyalty and frequency)\n",
    "df_eng['churn_risk_score'] = (0.5 * (1 - df_scaled['loyalty_scaled']) +\n",
    "                              0.5 * (1 - df_scaled['frequency_scaled']))\n",
    "\n",
    "print(\"\\n--- Newly created composite scores ---\")\n",
    "print(df_eng[['user_id', 'customer_value_score', 'churn_risk_score']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967f164",
   "metadata": {},
   "source": [
    "**5. Creating Interaction and Segmentation Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85dffb4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rationale: Create binary flags for easy filtering and to identify high-value customer segments like 'Champions'.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Binary Segmentation Flags (based on top 25% percentile)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m high_value_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mdf_eng\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.75\u001b[39m)\n\u001b[0;32m      5\u001b[0m high_loyalty_threshold \u001b[38;5;241m=\u001b[39m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloyalty_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.75\u001b[39m)\n\u001b[0;32m      6\u001b[0m high_frequency_threshold \u001b[38;5;241m=\u001b[39m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.75\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_eng' is not defined"
     ]
    }
   ],
   "source": [
    "# Rationale: Create binary flags for easy filtering and to identify high-value customer segments like 'Champions'.\n",
    "\n",
    "# Binary Segmentation Flags (based on top 25% percentile)\n",
    "high_value_threshold = df_eng['purchase_amount'].quantile(0.75)\n",
    "high_loyalty_threshold = df_eng['loyalty_score'].quantile(0.75)\n",
    "high_frequency_threshold = df_eng['purchase_frequency'].quantile(0.75)\n",
    "\n",
    "df_eng['is_high_value'] = (df_eng['purchase_amount'] > high_value_threshold).astype(int)\n",
    "df_eng['is_loyal'] = (df_eng['loyalty_score'] > high_loyalty_threshold).astype(int)\n",
    "df_eng['is_frequent'] = (df_eng['purchase_frequency'] > high_frequency_threshold).astype(int)\n",
    "df_eng['is_champion'] = (df_eng['is_high_value'] * df_eng['is_loyal'] * df_eng['is_frequent']).astype(int)\n",
    "\n",
    "print(\"\\n--- Binary Segmentation Flags ---\")\n",
    "print(df_eng[['user_id', 'is_high_value', 'is_loyal', 'is_frequent', 'is_champion']].head())\n",
    "print(f\"Number of Champion Customers: {df_eng['is_champion'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f23cc3",
   "metadata": {},
   "source": [
    "**6. Creating Statistical and Business-Savvy Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "664bc616",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rationale: Create normalized ranks and business-oriented scores like 'Growth Potential'.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Percentile Ranks\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincome_percentile\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_eng\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannual_income\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrank(pct\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspending_percentile\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_eng[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrank(pct\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Growth Potential Score (High Income, Relatively Low Spending)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_eng' is not defined"
     ]
    }
   ],
   "source": [
    "# Rationale: Create normalized ranks and business-oriented scores like 'Growth Potential'.\n",
    "\n",
    "# Percentile Ranks\n",
    "df_eng['income_percentile'] = df_eng['annual_income'].rank(pct=True)\n",
    "df_eng['spending_percentile'] = df_eng['purchase_amount'].rank(pct=True)\n",
    "\n",
    "# Growth Potential Score (High Income, Relatively Low Spending)\n",
    "df_eng['growth_potential_score'] = df_eng['income_percentile'] - df_eng['spending_percentile']\n",
    "\n",
    "print(\"\\n--- Growth Potential & Percentile Scores ---\")\n",
    "print(df_eng.sort_values('growth_potential_score', ascending=False)[['user_id', 'annual_income', 'purchase_amount', 'growth_potential_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cd7e2",
   "metadata": {},
   "source": [
    "**7. Finalizing the Model-Ready DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2b21b1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'region_dummies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 21\u001b[0m\n\u001b[0;32m      4\u001b[0m features_for_modeling \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# --- Core Scores ---\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannual_income\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m ]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Dynamically add the one-hot encoded region columns to the list\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m final_feature_list \u001b[38;5;241m=\u001b[39m features_for_modeling \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mregion_dummies\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     23\u001b[0m df_model_ready \u001b[38;5;241m=\u001b[39m df_eng[final_feature_list]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- FINAL MODEL-READY DATAFRAME ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'region_dummies' is not defined"
     ]
    }
   ],
   "source": [
    "# Rationale: Create a final, clean DataFrame containing only the identifier and the best engineered features for modeling.\n",
    "# This prevents data leakage and removes redundant columns.\n",
    "\n",
    "features_for_modeling = [\n",
    "    'user_id',\n",
    "    # --- Core Scores ---\n",
    "    'customer_value_score',\n",
    "    'churn_risk_score',\n",
    "    'growth_potential_score',\n",
    "    # --- Behavioral Ratios ---\n",
    "    'spend_per_purchase',\n",
    "    'income_to_spend_ratio',\n",
    "    # --- Key Segments/Flags ---\n",
    "    'is_champion',\n",
    "    # --- Raw Demographics (for direct use) ---\n",
    "    'age',\n",
    "    'annual_income'\n",
    "]\n",
    "\n",
    "# Dynamically add the one-hot encoded region columns to the list\n",
    "final_feature_list = features_for_modeling + list(region_dummies.columns)\n",
    "\n",
    "df_model_ready = df_eng[final_feature_list].copy()\n",
    "\n",
    "print(\"\\n--- FINAL MODEL-READY DATAFRAME ---\")\n",
    "print(\"Shape:\", df_model_ready.shape)\n",
    "print(\"Columns:\", df_model_ready.columns.tolist())\n",
    "print(df_model_ready.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcb05a",
   "metadata": {},
   "source": [
    "**7. Segmentation features - (is_high_value, is_loyal, is_frequent, customer_tier)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b26db1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_high_value\n",
    "df['spender'] = ['is_high_value' if x >= df['purchase_amount'].quantile(0.75) else 'high_value_in_progress' \n",
    "                 for x in df['purchase_amount']]\n",
    "#df['spender'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f81de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_loyal\n",
    "df['loyal'] = ['is_loyal' if x >= df['loyalty_score'].quantile(0.75) else 'loyalty_in_progress' \n",
    "                 for x in df['loyalty_score']]\n",
    "#df['loyal'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaf52686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_frequent\n",
    "df['frequent'] = ['is_frequent' if x >= df['purchase_frequency'].quantile(0.75) else 'frequency_in_progress' \n",
    "                 for x in df['purchase_frequency']]\n",
    "#df['frequent'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9349f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_tier. Would this be market segmentations? Would it come out from clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb4f08",
   "metadata": {},
   "source": [
    "**8. Demographic behavioural interaction - (young_high_spender, senior_loyal, income_age_segment, etc..)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic behavioural interaction. Would this be market segmentations? Would it come out from clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b0eb6",
   "metadata": {},
   "source": [
    "**9. Statistical Features - (frequency_percentile, is_outlier_spender, loyalty_deviation, etc..)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c747da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency_percentile\n",
    "df['frequency_percentile'] = ['is_frequent' if x >= df['purchase_frequency'].quantile(0.75) else 'frequency_in_progress' \n",
    "                 for x in df['purchase_frequency']]\n",
    "df['frequency_percentile'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b30094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({'purchase_amount': [120, 250, 75, 300, 180, 220, 90, 310]})\n",
    "\n",
    "# Define labels for each quantile\n",
    "labels = ['0-25%', '25-50%', '50-75%', '75-100%']\n",
    "\n",
    "# Create 'spender_group' column with labels\n",
    "df['spender_group'] = pd.qcut(df['purchase_amount'], q=4, labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
